{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    test_data = json.load(data_file)\n",
    "\n",
    "X_test = []\n",
    "ID_test = []\n",
    "for item in test_data:\n",
    "    X_test.append(', '.join(item['ingredients']))\n",
    "    ID_test.append(item['id'])    \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# \n",
    "# Feature Engineering\n",
    "\n",
    "There are likely many other features which can be created based on the contents of the recipes which may help us classify them.  We'll look at some things like the number of ingredients, if any canned or boxed goods are used, and for specific brands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Dataframe\n",
    "df_train = pd.DataFrame({\"recipe\": X_train, \"label\": y_train, \"ID\": None, \"source\": \"train\"})\n",
    "df_dev = pd.DataFrame({\"recipe\": X_dev, \"label\": y_dev, \"ID\": None, \"source\": \"dev\"})\n",
    "df_test = pd.DataFrame({\"recipe\": X_test, \"label\": None, \"ID\": ID_test, \"source\": \"test\"})\n",
    "df = pd.concat([df_train, df_dev, df_test])\n",
    "\n",
    "df[\"num_ingredients\"] = df[\"recipe\"].apply(lambda r: len(r.split(\", \")))\n",
    "df[\"canned\"] = df[\"recipe\"].apply(lambda r: int(\"can\" in r.lower()))\n",
    "#df[\"box\"] = df[\"recipe\"].apply(lambda r: \"\" in r.lower())\n",
    "\n",
    "brands = [\"kraft\"]\n",
    "df[\"brands\"] = df[\"recipe\"].apply(lambda r: (any([brand in r.lower() for brand in brands])))\n",
    "\n",
    "\n",
    "def get_average_ingredient_length(recipe):\n",
    "    \"\"\"This function returns the average number of words in a recipe\"\"\"\n",
    "    return np.mean(map(len, recipe.split(\", \")))\n",
    "df[\"ingredient_length\"] = df[\"recipe\"].apply(get_average_ingredient_length)\n",
    "\n",
    "\n",
    "print df.head()\n",
    "print \"\\n\\n\"\n",
    "engineered_columns = [\"canned\", \"num_ingredients\", \"ingredient_length\", \"brands\"]\n",
    "print df.loc[df[\"source\"] == \"train\" , engineered_columns + [\"label\"]].groupby(\"label\").agg([\"mean\", \"std\"])\n",
    "\n",
    "df_train = df.loc[df[\"source\"] == \"train\",]\n",
    "df_dev = df.loc[df[\"source\"] == \"dev\",]\n",
    "df_test = df.loc[df[\"source\"] == \"test\",]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Attemps\n",
    "\n",
    "# ItemSelector copied from official scikit-learn examples\n",
    "# http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html#sphx-glr-auto-examples-hetero-feature-union-py\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to scikit-learn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    #def transform(self, data_dict):\n",
    "        #if len(self.key) == 1:\n",
    "            #return list(data_dict[self.key])\n",
    "        #else:\n",
    "            #return data_dict.loc[:,self.key]\n",
    "    def transform(self, data_dict):\n",
    "       return data_dict.loc[:,self.key]\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    \"\"\"Transforms a sparse matrix into a dense matrix\"\"\"\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "class Printer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        print \"fiting self!\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        print data_dict.shape\n",
    "        print \"transforming\", data_dict\n",
    "        return data_dict\n",
    "\n",
    "    def fit_transform(self, data_dict, labels=None):\n",
    "        print \"doing print transform\"\n",
    "        print data_dict.shape\n",
    "        return list(data_dict)\n",
    "\n",
    "vectorizer = CountVectorizer()#preprocessor = custom_preprocessor,\n",
    "                             #tokenizer = custom_tokenizer)\n",
    "\n",
    "model = LogisticRegression()\n",
    "printer = Printer()\n",
    "\n",
    "pipe_base = Pipeline([\n",
    "                      (\"selector\", ItemSelector(key = [\"recipe\"])),\n",
    "                      (\"printer\", printer),\n",
    "                      (\"vectorizer\", vectorizer),\n",
    "                      (\"printer2\", printer),\n",
    "                      (\"model\", model)\n",
    "                      ])\n",
    "\n",
    "#pipe_base.fit(list(df_train[\"recipe\"]), df_train[\"label\"])\n",
    "\n",
    "pipe = Pipeline([\n",
    "     (\"union\", FeatureUnion([\n",
    "         (\"Ingredients\", Pipeline([\n",
    "             (\"selector\", ItemSelector(key = [\"recipe\"])),\n",
    "             (\"vectorizer\", vectorizer)\n",
    "             #(\"to_dense\", DenseTransformer())\n",
    "\n",
    "         ])),\n",
    "         # No transformations on other features, just select them.\n",
    "         (\"selector\", ItemSelector(key = engineered_columns))\n",
    "         ], \n",
    "         ))\n",
    "      ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "name": "Feature_Engineering.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
