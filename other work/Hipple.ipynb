{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y =[]\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "with open('test.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X_test = []\n",
    "ID_test = []\n",
    "for item in data:\n",
    "    X_test.append(', '.join(item['ingredients']))\n",
    "    ID_test.append(item['id'])    \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "              brazilian  british  cajun_creole  chinese  filipino  french  \\\n",
    "brazilian            46        1            11        0         1       4   \n",
    "british               0       72             4        1         2      33   \n",
    "cajun_creole          0        7           284        1         0       6   \n",
    "chinese               0        3             6      583         8       1   \n",
    "filipino              4        4             4       42        73       3   \n",
    "french                1       29            15        1         1     347   \n",
    "greek                 0        5             6        0         0       8   \n",
    "indian                2       13             2        3         1       4   \n",
    "irish                 1       17             5        0         3      21   \n",
    "italian               2       31            29        2         0     145   \n",
    "jamaican              4        3             4        1         2       1   \n",
    "japanese              1        1             0       48         1      12   \n",
    "korean                1        0             0       54         0       1   \n",
    "mexican               8        9            15        3         6       7   \n",
    "moroccan              0        4             3        0         0       2   \n",
    "russian               0       12             6        0         3      23   \n",
    "southern_us           3       18           108        9         8      32   \n",
    "spanish               0        6            13        1         0      28   \n",
    "thai                  4        0             1       30         2       0   \n",
    "vietnamese            3        1             0       18         2       2   \n",
    "\n",
    "              greek  indian  irish  italian  jamaican  japanese  korean  \\\n",
    "brazilian         0       7      0        5         1         0       0   \n",
    "british           1       6      9        7         1         1       1   \n",
    "cajun_creole      0       4      1       16         1         1       1   \n",
    "chinese           0       4      0        1         2        11      12   \n",
    "filipino          0       3      0        5         0         1       4   \n",
    "french           19       2      8      102         1         1       0   \n",
    "greek           165       5      0       38         0         0       0   \n",
    "indian           13     653      1        5         1         2       0   \n",
    "irish             1       1     62        9         0         0       0   \n",
    "italian          40       1      4     1559         0         0       0   \n",
    "jamaican          0       7      0        3        74         0       1   \n",
    "japanese          1      29      1        2         0       208      10   \n",
    "korean            0       0      0        1         0         6     145   \n",
    "mexican           4      16      2       20         1         0       2   \n",
    "moroccan          9      14      1        1         0         1       0   \n",
    "russian           2       2      3       10         0         0       1   \n",
    "southern_us       5      11      5       48         7         3       1   \n",
    "spanish           6       0      0       35         1         0       0   \n",
    "thai              1      11      0        1         0         3       2   \n",
    "vietnamese        0       0      0        3         0         3       2   \n",
    "\n",
    "              mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
    "brazilian           6         1        1           19       10     8   \n",
    "british             1         0        2           63        2     0   \n",
    "cajun_creole        8         1        1           40        4     0   \n",
    "chinese             2         1        2            6        0    15   \n",
    "filipino            4         0        1           28        2    11   \n",
    "french              7         3        2           76       20     0   \n",
    "greek               1        11        3            9        7     0   \n",
    "indian              5        19        1           15        0    17   \n",
    "irish               1         2        0           48        3     1   \n",
    "italian            16         9        2           84       34     4   \n",
    "jamaican            2         4        0           15        0     2   \n",
    "japanese            2         0        1           14        0    10   \n",
    "korean              2         0        0            8        0     2   \n",
    "mexican          1464         3        1           72       22    10   \n",
    "moroccan            1       167        0            7        3     1   \n",
    "russian            10         2       39           18        2     0   \n",
    "southern_us        40         2        6          728       12     6   \n",
    "spanish            12         3        0           25      115     2   \n",
    "thai                2         1        0            2        0   311   \n",
    "vietnamese          1         1        0            1        0    57   \n",
    "\n",
    "              vietnamese  \n",
    "brazilian              0  \n",
    "british                0  \n",
    "cajun_creole           0  \n",
    "chinese               13  \n",
    "filipino               1  \n",
    "french                 1  \n",
    "greek                  0  \n",
    "indian                 1  \n",
    "irish                  0  \n",
    "italian                1  \n",
    "jamaican               0  \n",
    "japanese               1  \n",
    "korean                 1  \n",
    "mexican                3  \n",
    "moroccan               1  \n",
    "russian                0  \n",
    "southern_us            4  \n",
    "spanish                0  \n",
    "thai                  20  \n",
    "vietnamese           101  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'peanuts, salt, water',\n u'large eggs, all-purpose flour, baking soda, buttermilk, yellow corn meal, dried sage, double-acting baking powder, unsalted butter, salt',\n u'pepper, jalapeno chilies, salt, oil, masa harina, lime juice, chili powder, cayenne pepper, onions, water, Mexican oregano, salsa, ground beef, fresh cilantro, garlic, peanut oil, cumin',\n u'peeled fresh ginger, noodles, hoisin sauce, salt, water, green onions, five-spice powder, pork tenderloin, peanut oil',\n u'short-grain rice, hot water, soy sauce, scallions, ponzu, cabbage, fresh ginger, shrimp']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "The count vectorizor is splitting on every space, not just on the \", \" that we'd prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'light_brown_sugar, lemongrass, jalapeno_chilies, cilantro_leaves, vegan_mayonnaise, soy_sauce, mo_hanh, cilantro, ground_white_pepper, liquid_aminos, pickled_carrots, vegetable_oil, ground_coriander, baguette, extra_firm_tofu, garlic, cucumber',\n u'ground_cinnamon, soy_sauce, raisins, brown_sugar, water, salt, chestnuts, pinenuts, jujube, sweet_rice, sesame_oil, walnuts',\n u'dry_white_wine, dry_bread_crumbs, olive_oil, garlic, fresh_parsley, lemon, fillets, ground_black_pepper, salt',\n u'filet_mignon, large_eggs, puff_pastry_sheets, veal_demi-glace, shallots, minced_garlic, mushrooms, Madeira, unsalted_butter, gorgonzola',\n u'scallions, light_coconut_milk, salt, water, basmati_rice']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_spaces(recipe_list):\n",
    "    return \", \".join(map(lambda s: s.replace(\" \", \"_\"), recipe_list.split(\", \")))\n",
    "\n",
    "X_train_nospc = list(map(remove_spaces, X_train))\n",
    "X_dev_nospc = list(map(remove_spaces, X_dev))\n",
    "X_train_nospc[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cv_nospc = CountVectorizer()\n",
    "tf_X_train_nospc= cv_nospc.fit_transform(X_train_nospc)\n",
    "tf_X_dev_nospc= cv_nospc.transform(X_dev_nospc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#[print(feat) for feat in cv_nospc.get_feature_names() if \"_\" in feat]\n",
    "\n",
    "#fails = [feat for feat in cv_nospc.get_feature_names() if feat[0] == \"_\"]\n",
    "#print fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(tf_X_train_nospc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71755656988693173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_dev, mnb.predict(tf_X_dev_nospc),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Well, taking out the spaces was no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Adding number of ingredients as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Preprocess step for working adjusting the ingredients list in comma\n",
    "    seperated format\n",
    "    \"\"\"\n",
    "\n",
    "    # Add number of ingredients as feature\n",
    "    s = s.lower()\n",
    "    num_ingredients = len(s.split(\", \"))\n",
    "    s += \", \" + str(num_ingredients)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72357435544647797"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_count = CountVectorizer(preprocessor = custom_preprocessor)\n",
    "mnb_with_count = MultinomialNB()\n",
    "\n",
    "tf_X_train_count = cv_count.fit_transform(X_train)\n",
    "tf_X_dev_count = cv_count.transform(X_dev)\n",
    "mnb_with_count.fit(tf_X_train_count, y_train) \n",
    "preds_with_count = mnb_with_count.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77598079133209752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(tf_X_train_count, y_train)\n",
    "lr_preds_with_count = lr.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, lr_preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Margininal improvement if any.\n",
    "Including the remove spaces function in the prepropocessor only hurts the accuracy by about 1-2% for both mnb and lr models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def custom_tokenizer_2(string):\n",
    "    result = []\n",
    "    \n",
    "    #overall note: the point of sorting the ingredients before adding\n",
    "    #them to the list is to prevent duplicates that are just flipped\n",
    "    #like \"unsalted butter\" and \"butter unsalted\"\n",
    "    \n",
    "    #create an empty list where we're going to put the ngrams\n",
    "    #where n = 1 so we can later create combinations of those\n",
    "    single_grams = []\n",
    "    \n",
    "    \n",
    "    for ingredient in string.split(', '):\n",
    "        for n in range(1,len(ingredient.split())+1):\n",
    "            grams = ngrams(ingredient.split(' '), n)\n",
    "            for gram in grams:\n",
    "                #if the length of the ngram we're looking at is 1,\n",
    "                #add it to our single grams list.\n",
    "                if n == 1:\n",
    "                    single_grams.append(gram[0])\n",
    "                result.append(\" \".join(sorted(list(gram))))\n",
    "    \n",
    "    #finally add every combination of the n = 1 ngrams\n",
    "    #so from ['unsalted butter', 'baking powder']\n",
    "    #we should be adding: 'butter unsalted', 'baking powder',\n",
    "    #'baking butter', 'baking unsalted', 'butter powder', 'powder unsalted'\n",
    "    for combo in combinations(single_grams, 2):\n",
    "        result.append(' '.join(sorted(list(combo))))\n",
    "    \n",
    "    #return the unique elements of this list\n",
    "    #since there will be plenty of duplicates\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(ingredients):\n",
    "    result = []\n",
    "    for ingredient in ingredients.split(', '):\n",
    "        temp = ingredient.lower()\n",
    "        \n",
    "        temp = re.sub(r'\\d+|&', '', temp)\n",
    "        temp = re.sub(r' +', ' ', temp)\n",
    "        temp = ' '.join(word for word in temp.split() if len(word)>2)\n",
    "        \n",
    "        result.append(\"\".join(temp))\n",
    "    \n",
    "    return \", \".join(result)\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    return string.split(', ') + re.split(', | ',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786202735318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782105262301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "                             \n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vectorize\", vectorizer), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "print(pipe.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Our biggest issues are southern_us and italian foods.\n",
    "\n",
    "Not with eachother, but Creole and southern are very simmilar and iatlian is also similar to a lot of stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# British-Irish Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817\n"
     ]
    }
   ],
   "source": [
    "BI_indexes = [True if y == u\"british\" or y == u\"irish\" else False for y in y_train]\n",
    "BI_indexes = pd.Series(BI_indexes)\n",
    "\n",
    "BI_y = pd.Series(y_train)[BI_indexes]\n",
    "BI_x = pd.Series(X_train)[BI_indexes]\n",
    "\n",
    "X_train_BI, X_dev_BI, y_train_BI, y_dev_BI = train_test_split(BI_x, BI_y)\n",
    "print len(X_train_BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    The dense transformer is required when the model being fit requires a\n",
    "    dense representation matrix, not a sparse one.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783882783883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.77      0.88      0.82       153\n",
      "      irish       0.81      0.66      0.73       120\n",
      "\n",
      "avg / total       0.79      0.78      0.78       273\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779983816556\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  (\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n",
    "print(classification_report(y_dev_BI, preds_BI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.85      0.87      0.86       153\n",
      "      irish       0.83      0.80      0.81       120\n",
      "\n",
      "avg / total       0.84      0.84      0.84       273\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83850179345\n",
      "0.838827838828"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty = \"l2\")\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n",
    "print(classification_report(y_dev_BI, preds_BI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Now we have to train a model to predict the british-irish category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29830\n",
      "29830\n"
     ]
    }
   ],
   "source": [
    "def make_group_labels(label, group_name, group_cuisines):\n",
    "    if label in group_cuisines:\n",
    "        return group_name\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "y_train_non_BI = pd.Series(y_train).map(lambda r: make_group_labels(r, \"BI\", [\"british\", \"irish\"]))\n",
    "y_dev_non_BI = pd.Series(y_dev).map(lambda r: make_group_labels(r, \"BI\", [\"british\", \"irish\"]))\n",
    "print len(y_train_non_BI)\n",
    "print len(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807320997586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804034698328\n"
     ]
    }
   ],
   "source": [
    "# Fit a model to classifier everything with joined British and Irish\n",
    "model = LogisticRegression(penalty = \"l2\")\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "pipe_non_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_non_BI.fit(X_train, y_train_non_BI)\n",
    "preds_non_BI = pipe_non_BI.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev_non_BI, preds_non_BI, average='weighted'))\n",
    "print(pipe_non_BI.score(X_dev, y_dev_non_BI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799793452032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.83      0.61      0.70       121\n",
      "     british       0.56      0.54      0.55       206\n",
      "cajun_creole       0.79      0.74      0.76       376\n",
      "     chinese       0.85      0.85      0.85       670\n",
      "    filipino       0.73      0.65      0.69       190\n",
      "      french       0.64      0.66      0.65       636\n",
      "       greek       0.79      0.72      0.75       258\n",
      "      indian       0.87      0.91      0.89       758\n",
      "       irish       0.60      0.55      0.57       175\n",
      "     italian       0.81      0.90      0.85      1963\n",
      "    jamaican       0.88      0.68      0.77       123\n",
      "    japanese       0.82      0.74      0.78       342\n",
      "      korean       0.86      0.77      0.82       221\n",
      "     mexican       0.91      0.93      0.92      1668\n",
      "    moroccan       0.84      0.78      0.81       215\n",
      "     russian       0.75      0.44      0.55       133\n",
      " southern_us       0.74      0.77      0.76      1056\n",
      "     spanish       0.67      0.45      0.53       247\n",
      "        thai       0.82      0.82      0.82       391\n",
      "  vietnamese       0.74      0.65      0.69       195\n",
      "\n",
      " avg / total       0.80      0.80      0.80      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On the British-Irish predicted recipies, predict with the BI model.\n",
    "BI_prediction_indexes = [True if y == u\"BI\" else False for y in preds_non_BI]\n",
    "fill_in_BI = pipe_BI.predict(pd.Series(X_dev)[BI_prediction_indexes])\n",
    "\n",
    "# Reconnect the predictions for the final model.\n",
    "preds_refilled = pd.Series(preds_non_BI)\n",
    "preds_refilled[BI_prediction_indexes] = fill_in_BI\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds_refilled, average='weighted'))\n",
    "print(classification_report(y_dev, preds_refilled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Run this ensemble model on the test data and make a submission csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "preds_test_base = pipe_non_BI.predict(X_test)\n",
    "BI_prediction_indexes_test = [True if y == u\"BI\" else False for y in preds_test_base]\n",
    "fill_in_BI_test = pipe_BI.predict(pd.Series(X_test)[BI_prediction_indexes_test])\n",
    "\n",
    "# Reconnect the predictions for the final model.\n",
    "preds_refilled_test = pd.Series(preds_test_base)\n",
    "preds_refilled_test[BI_prediction_indexes_test] = fill_in_BI_test\n",
    "\n",
    "\n",
    "submission = pd.DataFrame({\"id\":ID_test, \"cuisine\":preds_refilled_test})\n",
    "cols = [\"id\", \"cuisine\"]\n",
    "submission[cols].to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\":ID_test, \"cuisine\":preds_refilled_test})\n",
    "cols = [\"id\", \"cuisine\"]\n",
    "submission[cols].to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (21, 21), indices imply (19, 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1dd2b24f4c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_dev_non_BI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds_non_BI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcmdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcats_BI\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcats_BI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(cmdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[1;32m--> 306\u001b[1;33m                                          copy=copy)\n\u001b[0m\u001b[0;32m    307\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_init_ndarray\u001b[1;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   4301\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4302\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4303\u001b[1;33m         \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Chris\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   4278\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4279\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[1;32m-> 4280\u001b[1;33m         passed, implied))\n\u001b[0m\u001b[0;32m   4281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (21, 21), indices imply (19, 19)"
     ]
    }
   ],
   "source": [
    "cats_BI = ['BI', 'brazilian', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "cm = confusion_matrix(y_dev_non_BI, preds_non_BI)\n",
    "cmdf = pd.DataFrame(cm, index = cats_BI, columns = cats_BI)\n",
    "#print(cmdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Contiential food Source\n",
    "\n",
    "Here I attempt to improve the model by adding which continent the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "continents ={'brazilian':\"America\"\n",
    "             , 'british': \"Europe\"\n",
    "             , 'cajun_creole': \"America\"\n",
    "             , 'chinese': \"Asia\"\n",
    "             , 'filipino': \"Asia\"\n",
    "             , 'french': \"Europe\"\n",
    "             , 'greek': \"Europe\"\n",
    "             , 'indian': \"Asia\"\n",
    "             ,'irish': \"Europe\"\n",
    "             , 'italian': \"Europe\"\n",
    "             , 'jamaican': \"America\"\n",
    "             ,'japanese': \"Asia\"\n",
    "             , 'korean': \"Asia\"\n",
    "             , 'mexican': \"America\"\n",
    "             , 'moroccan': \"Africa\"\n",
    "             , 'russian': \"Europe\"\n",
    "             , 'southern_us':\"America\"\n",
    "             , 'spanish': \"Europe\"\n",
    "             , 'thai': \"Asia\"\n",
    "             ,'vietnamese': \"Asia\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "continents[\"chinese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def find_canned_goods(recipe):\n",
    "    \"\"\"This function searches for anything that's canned in a recipie\"\"\"\n",
    "    if \"can\" in recipe.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X_train = pd.DataFrame({\"Recipe\": X_train, \"Label\": y_train})\n",
    "df_X_train[\"Continent\"] = [continents[label] for label in df_X_train[\"Label\"]]\n",
    "df_X_train[\"Length\"] = df_X_train[\"Recipe\"].apply(lambda r: len(r.split(\", \")))\n",
    "df_X_train[\"Canned\"] = df_X_train[\"Recipe\"].apply(find_canned_goods)\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "BI_train = df_X_train.loc[BI_indexes]\n",
    "BI_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_X_train[[\"Label\", \"Canned\", \"Length\"]].groupby(\"Label\").agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "All of the categories are within 1 standard deviation of eachother from all the other categories.  Unlikely to be any predictive power in these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799793452032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.83      0.61      0.70       121\n",
      "     british       0.56      0.54      0.55       206\n",
      "cajun_creole       0.79      0.74      0.76       376\n",
      "     chinese       0.85      0.85      0.85       670\n",
      "    filipino       0.73      0.65      0.69       190\n",
      "      french       0.64      0.66      0.65       636\n",
      "       greek       0.79      0.72      0.75       258\n",
      "      indian       0.87      0.91      0.89       758\n",
      "       irish       0.60      0.55      0.57       175\n",
      "     italian       0.81      0.90      0.85      1963\n",
      "    jamaican       0.88      0.68      0.77       123\n",
      "    japanese       0.82      0.74      0.78       342\n",
      "      korean       0.86      0.77      0.82       221\n",
      "     mexican       0.91      0.93      0.92      1668\n",
      "    moroccan       0.84      0.78      0.81       215\n",
      "     russian       0.75      0.44      0.55       133\n",
      " southern_us       0.74      0.77      0.76      1056\n",
      "     spanish       0.67      0.45      0.53       247\n",
      "        thai       0.82      0.82      0.82       391\n",
      "  vietnamese       0.74      0.65      0.69       195\n",
      "\n",
      " avg / total       0.80      0.80      0.80      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BI_group = {\"cuisines\": [\"british\", \"irish\"], #which cuisines are included\n",
    "            \"group_name\": \"BI\", # what sub-group this called by base model\n",
    "            \"model\": pipe_BI} # The model which predicts the final cuisines\n",
    "\n",
    "def predict_ensemble_model(base_model, splits, data):\n",
    "    preds = base_model.predict(data)\n",
    "    for split in splits:\n",
    "        split_prediction_indexes = [True if y == split['group_name'] else False for y in preds]\n",
    "        fill_in_split = split['model'].predict(pd.Series(data)[split_prediction_indexes])\n",
    "\n",
    "        # Reconnect the predictions for the final model.\n",
    "        preds= pd.Series(preds)\n",
    "        preds[split_prediction_indexes] = fill_in_split\n",
    "    return preds\n",
    "\n",
    "final_preds = predict_ensemble_model(pipe_non_BI, [BI_group], X_dev)\n",
    "\n",
    "\n",
    "\n",
    "print(metrics.f1_score(y_dev, final_preds, average='weighted'))\n",
    "print(classification_report(y_dev, final_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions = predict_ensemble_model(pipe_non_BI, [BI_group], X_test)\n",
    "submission = pd.DataFrame({\"id\":ID_test, \"cuisine\":test_predictions})\n",
    "cols = [\"id\", \"cuisine\"]\n",
    "submission[cols].to_csv(\"submission.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "BI_group = {\"cuisines\": [u\"british\", u\"irish\"], #which cuisines are included\n",
    "            \"group_name\": u\"BI\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "\n",
    "def train_ensemble_model(base_model, splits, X_train, y_train, X_dev, y_dev):\n",
    "    # First we must relabel the data to account for all of the splits\n",
    "    y_train_grouped = y_train\n",
    "    y_dev_grouped = y_dev\n",
    "    for split in splits:\n",
    "        y_train_grouped= pd.Series(y_train_grouped).map(lambda r:make_group_labels(r, split[\"group_name\"], split[\"cuisines\"]))\n",
    "        y_dev_grouped= pd.Series(y_dev_grouped).map(lambda r:make_group_labels(r, split[\"group_name\"], split[\"cuisines\"]))\n",
    "\n",
    "    # Next we train the base_model\n",
    "    print \"Fitting Base Model to classify into groups.\"\n",
    "    base_model.fit(X_train, y_train_grouped)\n",
    "\n",
    "    # Train the sub models for each group\n",
    "    for split in splits:\n",
    "        print(\"Training split model:\", split[\"group_name\"], \"for cuisines:\", split[\"cuisines\"])\n",
    "        # slice the data\n",
    "        train_split_indicies = pd.Series([True if y == split[\"group_name\"] else False for y in y_train_grouped])\n",
    "        #print split[\"cuisines\"]\n",
    "        #print y_train_grouped.value_counts()\n",
    "        #print train_split_indicies.value_counts()\n",
    "        X_train_split = pd.Series(X_train)[train_split_indicies]\n",
    "        y_train_split = pd.Series(y_train)[train_split_indicies]\n",
    "        #print(\"y_train_split.value_counts:\", y_train_split.value_counts())\n",
    "\n",
    "        dev_split_indicies = pd.Series([True if y == split[\"group_name\"] else False for y in y_dev_grouped])\n",
    "        X_dev_split = pd.Series(X_dev)[dev_split_indicies]\n",
    "        y_dev_split = pd.Series(y_dev)[dev_split_indicies]\n",
    "\n",
    "        if split[\"model\"] is None:\n",
    "            vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "            classifier = LogisticRegression(penalty = \"l2\")\n",
    "            split_model = Pipeline([(\"vectorize\", vectorizer),\n",
    "                                    (\"model\", classifier)])\n",
    "        else:\n",
    "           split_model = split[\"model\"]\n",
    "\n",
    "        split_model.fit(X_train_split, y_train_split)\n",
    "        split_predictions = split_model.predict(X_dev_split)\n",
    "        split[\"model\"] = split_model\n",
    "        #print(\"split_predictions:\", pd.Series(split_predictions).value_counts())\n",
    "        #print(\"y_dev_split:\", pd.Series(y_dev_split).value_counts())\n",
    "        #for i in range(5):\n",
    "            #print(split_predictions[i], list(y_dev_split)[i])\n",
    "\n",
    "        split[\"classification_report\"] = classification_report(y_dev_split, split_predictions)\n",
    "        #split[\"classification_score\"] = metrics.f1_score(y_dev_split, split_predictions)\n",
    "\n",
    "    # Make dev predictions and combine all models\n",
    "    print \"Creating final predictions.\"\n",
    "    preds = base_model.predict(X_dev)\n",
    "    for split in splits:\n",
    "        split_prediction_indicies = [True if y == split[\"group_name\"] else False for y in preds]\n",
    "        fill_in_predictions = split[\"model\"].predict(pd.Series(X_dev)[split_prediction_indicies])\n",
    "        #print(pd.Series(fill_in_predictions).value_counts())\n",
    "        preds = pd.Series(preds)\n",
    "        preds[split_prediction_indicies] = fill_in_predictions\n",
    "\n",
    "    print(metrics.f1_score(y_dev, preds, average=\"weighted\"))\n",
    "    print(classification_report(y_dev, preds))\n",
    "    return base_model, splits, preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787833390645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.86      0.52      0.65       121\n",
      "     british       0.62      0.44      0.52       206\n",
      "cajun_creole       0.82      0.72      0.77       376\n",
      "     chinese       0.82      0.86      0.84       670\n",
      "    filipino       0.67      0.68      0.68       190\n",
      "      french       0.59      0.68      0.63       636\n",
      "       greek       0.76      0.70      0.73       258\n",
      "      indian       0.88      0.89      0.88       758\n",
      "       irish       0.64      0.52      0.57       175\n",
      "     italian       0.79      0.91      0.84      1963\n",
      "    jamaican       0.89      0.64      0.75       123\n",
      "    japanese       0.86      0.71      0.77       342\n",
      "      korean       0.89      0.72      0.80       221\n",
      "     mexican       0.88      0.93      0.90      1668\n",
      "    moroccan       0.87      0.73      0.79       215\n",
      "     russian       0.61      0.41      0.49       133\n",
      " southern_us       0.77      0.74      0.76      1056\n",
      "     spanish       0.66      0.47      0.55       247\n",
      "        thai       0.79      0.81      0.80       391\n",
      "  vietnamese       0.70      0.63      0.66       195\n",
      "\n",
      " avg / total       0.79      0.79      0.79      9944\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.80      0.83      0.82       206\n",
      "      irish       0.80      0.76      0.78       175\n",
      "\n",
      "avg / total       0.80      0.80      0.80       381\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     french       0.73      0.75      0.74       636\n",
      "      greek       0.83      0.74      0.78       258\n",
      "    italian       0.86      0.92      0.89      1963\n",
      "    mexican       0.95      0.95      0.95      1668\n",
      "    russian       0.84      0.48      0.61       133\n",
      "    spanish       0.75      0.49      0.60       247\n",
      "\n",
      "avg / total       0.86      0.87      0.86      4905\n",
      "\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    chinese       0.90      0.92      0.91       670\n",
      "   filipino       0.82      0.82      0.82       190\n",
      "       thai       0.84      0.86      0.85       391\n",
      " vietnamese       0.75      0.66      0.70       195\n",
      "\n",
      "avg / total       0.85      0.86      0.85      1446\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training split model:', u'ASIA', 'for cuisines:', [u'chinese', u'vietnamese', u'thai', u'filipino'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training split model:', u'EU', 'for cuisines:', [u'spanish', u'italian', u'greek', u'french', u'soutern_us', u'russian', u'mexican'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training split model:', u'BI', 'for cuisines:', [u'british', u'irish'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Model to classify into groups.\n"
     ]
    }
   ],
   "source": [
    "# Define structure of base model.\n",
    "model = LogisticRegression(penalty = \"l2\")\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "base_model = Pipeline([(\"vectorize\", vectorizer),\n",
    "                       (\"model\", model)])\n",
    "\n",
    "BI_group = {\"cuisines\": [u\"british\", u\"irish\"], #which cuisines are included\n",
    "            \"group_name\": u\"BI\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "euro_group = {\"cuisines\": [u\"spanish\", u\"italian\", u\"greek\", u\"french\", u\"soutern_us\", u\"russian\", u\"mexican\"], #which cuisines are included\n",
    "            \"group_name\": u\"EU\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "\n",
    "# Not used\n",
    "south_group = {\"cuisines\": [u\"mexican\", u\"southern_us\", u\"cajun_creole\", u\"russian\"],\n",
    "            \"group_name\": u\"SOUTH\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "asia_group = {\"cuisines\": [u\"chinese\", u\"vietnamese\", u\"thai\", u\"filipino\"],\n",
    "            \"group_name\": u\"ASIA\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "\n",
    "splits = [BI_group, euro_group, asia_group]\n",
    "\n",
    "base_model, splits, preds = train_ensemble_model(base_model, splits, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "for split in splits:\n",
    "    print split[\"classification_report\"]\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brazilian  british  cajun_creole  chinese  filipino  french  \\\n",
      "brazilian            63        0             2        1         5       7   \n",
      "british               0       91             1        0         1      37   \n",
      "cajun_creole          0        4           271        2         2       9   \n",
      "chinese               0        1             2      577         9       1   \n",
      "filipino              3        1             0       12       130       4   \n",
      "french                0       10             5        1         2     432   \n",
      "greek                 0        2             0        0         1       7   \n",
      "indian                0        1             0        1         3       8   \n",
      "irish                 1       18             0        1         0      15   \n",
      "italian               0        2             1        3         5      80   \n",
      "jamaican              2        1             2        2         3       1   \n",
      "japanese              1        0             0       30         5       4   \n",
      "korean                0        0             0       25         6       1   \n",
      "mexican               2        1             5        3         3      14   \n",
      "moroccan              0        0             0        1         0       6   \n",
      "russian               0        6             0        0         1      21   \n",
      "southern_us           0        9            36        5         4      59   \n",
      "spanish               1        0             5        0         2      31   \n",
      "thai                  0        0             1       24         4       0   \n",
      "vietnamese            0        0             0       14         8       1   \n",
      "\n",
      "              greek  indian  irish  italian  jamaican  japanese  korean  \\\n",
      "brazilian         0       2      0        5         3         1       0   \n",
      "british           0       5     15       20         0         1       0   \n",
      "cajun_creole      0       0      0       24         2         1       0   \n",
      "chinese           0       5      1        5         0         9      15   \n",
      "filipino          0       1      0        7         1         1       0   \n",
      "french            3       3      8      116         0         1       0   \n",
      "greek           181       1      0       50         0         0       0   \n",
      "indian            9     672      0       13         1         5       0   \n",
      "irish             0       1     91       22         0         0       0   \n",
      "italian          27       5      4     1778         0         1       0   \n",
      "jamaican          0      10      2        5        79         0       0   \n",
      "japanese          0      26      1       11         0       242       2   \n",
      "korean            0       0      1        5         0        12     159   \n",
      "mexican           1       3      2       38         0         0       0   \n",
      "moroccan          5      11      2       11         1         0       0   \n",
      "russian           3       2      1       19         0         0       0   \n",
      "southern_us       5       3     13       61         2         4       1   \n",
      "spanish           3       1      2       57         0         0       0   \n",
      "thai              1      10      0        3         0         1       0   \n",
      "vietnamese        0       1      0        6         0         4       1   \n",
      "\n",
      "              mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
      "brazilian          17         0        1            6        3     5   \n",
      "british             3         0        5           26        1     0   \n",
      "cajun_creole       18         0        0           39        4     0   \n",
      "chinese             6         0        0            7        0    18   \n",
      "filipino            8         0        0           10        2     4   \n",
      "french             13         2        4           26       10     0   \n",
      "greek               3         5        1            3        3     0   \n",
      "indian             16         8        2            6        0    11   \n",
      "irish               0         0        4           19        3     0   \n",
      "italian            21         3        1           20       12     0   \n",
      "jamaican            3         0        1            9        1     2   \n",
      "japanese            5         0        1            5        0     5   \n",
      "korean              5         0        0            2        0     3   \n",
      "mexican          1556         3        1           24       11     1   \n",
      "moroccan           11       157        0            6        3     0   \n",
      "russian            12         1       55           11        1     0   \n",
      "southern_us        49         2        9          785        4     3   \n",
      "spanish            19         0        4            5      115     2   \n",
      "thai                5         0        1            3        0   318   \n",
      "vietnamese          4         0        0            1        0    32   \n",
      "\n",
      "              vietnamese  \n",
      "brazilian              0  \n",
      "british                0  \n",
      "cajun_creole           0  \n",
      "chinese               14  \n",
      "filipino               6  \n",
      "french                 0  \n",
      "greek                  1  \n",
      "indian                 2  \n",
      "irish                  0  \n",
      "italian                0  \n",
      "jamaican               0  \n",
      "japanese               4  \n",
      "korean                 2  \n",
      "mexican                0  \n",
      "moroccan               1  \n",
      "russian                0  \n",
      "southern_us            2  \n",
      "spanish                0  \n",
      "thai                  20  \n",
      "vietnamese           123  \n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_dev, preds)\n",
    "cmdf = pd.DataFrame(cm, index = cats, columns = cats)\n",
    "print cmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def classification_report_df(report):\n",
    "    \"\"\"https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format\"\"\"\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split()\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    return pd.DataFrame.from_dict(report_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              f1_score  precision  recall  support  count\nclass                                                    \nbrazilian         0.65       0.86    0.52    121.0    121\nbritish           0.52       0.62    0.44    206.0    206\ncajun_creole      0.77       0.82    0.72    376.0    376\nchinese           0.84       0.82    0.86    670.0    670\nfilipino          0.68       0.67    0.68    190.0    190\nfrench            0.63       0.59    0.68    636.0    636\ngreek             0.73       0.76    0.70    258.0    258\nindian            0.88       0.88    0.89    758.0    758\nirish             0.57       0.64    0.52    175.0    175\nitalian           0.84       0.79    0.91   1963.0   1963\njamaican          0.75       0.89    0.64    123.0    123\njapanese          0.77       0.86    0.71    342.0    342\nkorean            0.80       0.89    0.72    221.0    221\nmexican           0.90       0.88    0.93   1668.0   1668\nmoroccan          0.79       0.87    0.73    215.0    215\nrussian           0.49       0.61    0.41    133.0    133\nsouthern_us       0.76       0.77    0.74   1056.0   1056\nspanish           0.55       0.66    0.47    247.0    247\nthai              0.80       0.79    0.81    391.0    391\nvietnamese        0.66       0.70    0.63    195.0    195"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dev_report.join(pd.DataFrame(dev_counts), on=\"class\")\n",
    "#dev_report[\"counts\"] = dev_counts\n",
    "dev_counts = pd.Series(y_dev).value_counts()\n",
    "dev_counts = pd.DataFrame(dev_counts).reset_index()\n",
    "dev_counts.columns = [\"class\", \"count\"]\n",
    "dev_counts.set_index(\"class\", inplace=True)\n",
    "\n",
    "\n",
    "dev_report = classification_report_df(classification_report(y_dev, preds))\n",
    "dev_report.set_index(\"class\", inplace=True)\n",
    "dev_report = dev_report.join(dev_counts)\n",
    "dev_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d773ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x300c9ef0>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix, axes = plt.subplots(ncols = 3, nrows = 1, figsize = [15, 5])\n",
    "dev_report.plot(\"count\", \"precision\", kind = \"scatter\", ax = axes[0])\n",
    "dev_report.plot(\"count\", \"f1_score\", kind = \"scatter\", ax = axes[1])\n",
    "dev_report.plot(\"count\", \"recall\", kind = \"scatter\", ax = axes[2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": null,
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "Hipple.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
