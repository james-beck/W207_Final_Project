{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y =[]\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "              brazilian  british  cajun_creole  chinese  filipino  french  \\\n",
    "brazilian            46        1            11        0         1       4   \n",
    "british               0       72             4        1         2      33   \n",
    "cajun_creole          0        7           284        1         0       6   \n",
    "chinese               0        3             6      583         8       1   \n",
    "filipino              4        4             4       42        73       3   \n",
    "french                1       29            15        1         1     347   \n",
    "greek                 0        5             6        0         0       8   \n",
    "indian                2       13             2        3         1       4   \n",
    "irish                 1       17             5        0         3      21   \n",
    "italian               2       31            29        2         0     145   \n",
    "jamaican              4        3             4        1         2       1   \n",
    "japanese              1        1             0       48         1      12   \n",
    "korean                1        0             0       54         0       1   \n",
    "mexican               8        9            15        3         6       7   \n",
    "moroccan              0        4             3        0         0       2   \n",
    "russian               0       12             6        0         3      23   \n",
    "southern_us           3       18           108        9         8      32   \n",
    "spanish               0        6            13        1         0      28   \n",
    "thai                  4        0             1       30         2       0   \n",
    "vietnamese            3        1             0       18         2       2   \n",
    "\n",
    "              greek  indian  irish  italian  jamaican  japanese  korean  \\\n",
    "brazilian         0       7      0        5         1         0       0   \n",
    "british           1       6      9        7         1         1       1   \n",
    "cajun_creole      0       4      1       16         1         1       1   \n",
    "chinese           0       4      0        1         2        11      12   \n",
    "filipino          0       3      0        5         0         1       4   \n",
    "french           19       2      8      102         1         1       0   \n",
    "greek           165       5      0       38         0         0       0   \n",
    "indian           13     653      1        5         1         2       0   \n",
    "irish             1       1     62        9         0         0       0   \n",
    "italian          40       1      4     1559         0         0       0   \n",
    "jamaican          0       7      0        3        74         0       1   \n",
    "japanese          1      29      1        2         0       208      10   \n",
    "korean            0       0      0        1         0         6     145   \n",
    "mexican           4      16      2       20         1         0       2   \n",
    "moroccan          9      14      1        1         0         1       0   \n",
    "russian           2       2      3       10         0         0       1   \n",
    "southern_us       5      11      5       48         7         3       1   \n",
    "spanish           6       0      0       35         1         0       0   \n",
    "thai              1      11      0        1         0         3       2   \n",
    "vietnamese        0       0      0        3         0         3       2   \n",
    "\n",
    "              mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
    "brazilian           6         1        1           19       10     8   \n",
    "british             1         0        2           63        2     0   \n",
    "cajun_creole        8         1        1           40        4     0   \n",
    "chinese             2         1        2            6        0    15   \n",
    "filipino            4         0        1           28        2    11   \n",
    "french              7         3        2           76       20     0   \n",
    "greek               1        11        3            9        7     0   \n",
    "indian              5        19        1           15        0    17   \n",
    "irish               1         2        0           48        3     1   \n",
    "italian            16         9        2           84       34     4   \n",
    "jamaican            2         4        0           15        0     2   \n",
    "japanese            2         0        1           14        0    10   \n",
    "korean              2         0        0            8        0     2   \n",
    "mexican          1464         3        1           72       22    10   \n",
    "moroccan            1       167        0            7        3     1   \n",
    "russian            10         2       39           18        2     0   \n",
    "southern_us        40         2        6          728       12     6   \n",
    "spanish            12         3        0           25      115     2   \n",
    "thai                2         1        0            2        0   311   \n",
    "vietnamese          1         1        0            1        0    57   \n",
    "\n",
    "              vietnamese  \n",
    "brazilian              0  \n",
    "british                0  \n",
    "cajun_creole           0  \n",
    "chinese               13  \n",
    "filipino               1  \n",
    "french                 1  \n",
    "greek                  0  \n",
    "indian                 1  \n",
    "irish                  0  \n",
    "italian                1  \n",
    "jamaican               0  \n",
    "japanese               1  \n",
    "korean                 1  \n",
    "mexican                3  \n",
    "moroccan               1  \n",
    "russian                0  \n",
    "southern_us            4  \n",
    "spanish                0  \n",
    "thai                  20  \n",
    "vietnamese           101  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'peanuts, salt, water',\n u'large eggs, all-purpose flour, baking soda, buttermilk, yellow corn meal, dried sage, double-acting baking powder, unsalted butter, salt',\n u'pepper, jalapeno chilies, salt, oil, masa harina, lime juice, chili powder, cayenne pepper, onions, water, Mexican oregano, salsa, ground beef, fresh cilantro, garlic, peanut oil, cumin',\n u'peeled fresh ginger, noodles, hoisin sauce, salt, water, green onions, five-spice powder, pork tenderloin, peanut oil',\n u'short-grain rice, hot water, soy sauce, scallions, ponzu, cabbage, fresh ginger, shrimp']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "The count vectorizor is splitting on every space, not just on the \", \" that we'd prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'light_brown_sugar, lemongrass, jalapeno_chilies, cilantro_leaves, vegan_mayonnaise, soy_sauce, mo_hanh, cilantro, ground_white_pepper, liquid_aminos, pickled_carrots, vegetable_oil, ground_coriander, baguette, extra_firm_tofu, garlic, cucumber',\n u'ground_cinnamon, soy_sauce, raisins, brown_sugar, water, salt, chestnuts, pinenuts, jujube, sweet_rice, sesame_oil, walnuts',\n u'dry_white_wine, dry_bread_crumbs, olive_oil, garlic, fresh_parsley, lemon, fillets, ground_black_pepper, salt',\n u'filet_mignon, large_eggs, puff_pastry_sheets, veal_demi-glace, shallots, minced_garlic, mushrooms, Madeira, unsalted_butter, gorgonzola',\n u'scallions, light_coconut_milk, salt, water, basmati_rice']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_spaces(recipe_list):\n",
    "    return \", \".join(map(lambda s: s.replace(\" \", \"_\"), recipe_list.split(\", \")))\n",
    "\n",
    "X_train_nospc = list(map(remove_spaces, X_train))\n",
    "X_dev_nospc = list(map(remove_spaces, X_dev))\n",
    "X_train_nospc[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cv_nospc = CountVectorizer()\n",
    "tf_X_train_nospc= cv_nospc.fit_transform(X_train_nospc)\n",
    "tf_X_dev_nospc= cv_nospc.transform(X_dev_nospc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#[print(feat) for feat in cv_nospc.get_feature_names() if \"_\" in feat]\n",
    "\n",
    "#fails = [feat for feat in cv_nospc.get_feature_names() if feat[0] == \"_\"]\n",
    "#print fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(tf_X_train_nospc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71755656988693173"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_dev, mnb.predict(tf_X_dev_nospc),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Well, taking out the spaces was no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Adding number of ingredients as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Preprocess step for working adjusting the ingredients list in comma\n",
    "    seperated format\n",
    "    \"\"\"\n",
    "\n",
    "    # Add number of ingredients as feature\n",
    "    s = s.lower()\n",
    "    num_ingredients = len(s.split(\", \"))\n",
    "    s += \", \" + str(num_ingredients)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72357435544647797"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_count = CountVectorizer(preprocessor = custom_preprocessor)\n",
    "mnb_with_count = MultinomialNB()\n",
    "\n",
    "tf_X_train_count = cv_count.fit_transform(X_train)\n",
    "tf_X_dev_count = cv_count.transform(X_dev)\n",
    "mnb_with_count.fit(tf_X_train_count, y_train) \n",
    "preds_with_count = mnb_with_count.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77598079133209752"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(tf_X_train_count, y_train)\n",
    "lr_preds_with_count = lr.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, lr_preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Margininal improvement if any.\n",
    "Including the remove spaces function in the prepropocessor only hurts the accuracy by about 1-2% for both mnb and lr models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def custom_tokenizer_2(string):\n",
    "    result = []\n",
    "    \n",
    "    #overall note: the point of sorting the ingredients before adding\n",
    "    #them to the list is to prevent duplicates that are just flipped\n",
    "    #like \"unsalted butter\" and \"butter unsalted\"\n",
    "    \n",
    "    #create an empty list where we're going to put the ngrams\n",
    "    #where n = 1 so we can later create combinations of those\n",
    "    single_grams = []\n",
    "    \n",
    "    \n",
    "    for ingredient in string.split(', '):\n",
    "        for n in range(1,len(ingredient.split())+1):\n",
    "            grams = ngrams(ingredient.split(' '), n)\n",
    "            for gram in grams:\n",
    "                #if the length of the ngram we're looking at is 1,\n",
    "                #add it to our single grams list.\n",
    "                if n == 1:\n",
    "                    single_grams.append(gram[0])\n",
    "                result.append(\" \".join(sorted(list(gram))))\n",
    "    \n",
    "    #finally add every combination of the n = 1 ngrams\n",
    "    #so from ['unsalted butter', 'baking powder']\n",
    "    #we should be adding: 'butter unsalted', 'baking powder',\n",
    "    #'baking butter', 'baking unsalted', 'butter powder', 'powder unsalted'\n",
    "    for combo in combinations(single_grams, 2):\n",
    "        result.append(' '.join(sorted(list(combo))))\n",
    "    \n",
    "    #return the unique elements of this list\n",
    "    #since there will be plenty of duplicates\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(ingredients):\n",
    "    result = []\n",
    "    for ingredient in ingredients.split(', '):\n",
    "        temp = ingredient.lower()\n",
    "        \n",
    "        temp = re.sub(r'\\d+|&', '', temp)\n",
    "        temp = re.sub(r' +', ' ', temp)\n",
    "        temp = ' '.join(word for word in temp.split() if len(word)>2)\n",
    "        \n",
    "        result.append(\"\".join(temp))\n",
    "    \n",
    "    return \", \".join(result)\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    return string.split(', ') + re.split(', | ',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786202735318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782105262301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "                             \n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vectorize\", vectorizer), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "print(pipe.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'brazilian',\n u'british',\n u'cajun_creole',\n u'chinese',\n u'filipino',\n u'french',\n u'greek',\n u'indian',\n u'irish',\n u'italian',\n u'jamaican',\n u'japanese',\n u'korean',\n u'mexican',\n u'moroccan',\n u'russian',\n u'southern_us',\n u'spanish',\n u'thai',\n u'vietnamese'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Our biggest issues are southern_us and italian foods.\n",
    "\n",
    "Not with eachother, but Creole and southern are very simmilar and iatlian is also similar to a lot of stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# British-Irish Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817\n"
     ]
    }
   ],
   "source": [
    "BI_indexes = [True if y == u\"british\" or y == u\"irish\" else False for y in y_train]\n",
    "BI_indexes = pd.Series(BI_indexes)\n",
    "\n",
    "BI_y = pd.Series(y_train)[BI_indexes]\n",
    "BI_x = pd.Series(X_train)[BI_indexes]\n",
    "\n",
    "X_train_BI, X_dev_BI, y_train_BI, y_dev_BI = train_test_split(BI_x, BI_y)\n",
    "print len(X_train_BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "      \"\"\"The dense transformer is required when the model being fit requires a\n",
    "      dense representation matrix, not a sparse one.\"\"\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81135554153\n",
      "0.813186813187\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  (\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808488612836\n",
      "0.809523809524\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Now we have to train a model to predict the british-irish category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29830\n",
      "29830\n"
     ]
    }
   ],
   "source": [
    "def make_BI_labels(label):\n",
    "    if label == u\"british\" or label == u\"irish\":\n",
    "        return u\"BI\"\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "y_train_non_BI = pd.Series(y_train).map(make_BI_labels)\n",
    "y_dev_non_BI = pd.Series(y_dev).map(make_BI_labels)\n",
    "print len(y_train_non_BI)\n",
    "print len(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789521319389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786399504888\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "\n",
    "pipe_non_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_non_BI.fit(X_train, y_train_non_BI)\n",
    "preds_non_BI = pipe_non_BI.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev_non_BI, preds_non_BI, average='weighted'))\n",
    "print(pipe_non_BI.score(X_dev, y_dev_non_BI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               brazilian  british-irish  cajun_creole  chinese  filipino  \\\n",
      "brazilian            208              1             2        1         1   \n",
      "british-irish          0             66             3        1         2   \n",
      "cajun_creole           6              1           269        2         0   \n",
      "chinese                3              0             4      569         8   \n",
      "filipino               5              4             1       15       117   \n",
      "french                25              0             5        0         2   \n",
      "greek                  0              0             1        0         0   \n",
      "indian                 6              2             1        2         2   \n",
      "italian               14              0             5        3         1   \n",
      "jamaican               1              2             2        2         1   \n",
      "japanese               2              1             1       30         3   \n",
      "korean                 1              0             0       29         4   \n",
      "mexican                1              4             6        4         3   \n",
      "moroccan               3              0             2        1         0   \n",
      "russian               15              0             3        0         1   \n",
      "southern_us           33              1            40        9         4   \n",
      "spanish                4              1             1        0         1   \n",
      "thai                   0              2             1       22         7   \n",
      "vietnamese             0              1             0       15         4   \n",
      "\n",
      "               french  greek  indian  italian  jamaican  japanese  korean  \\\n",
      "brazilian          53      2       8       24         1         1       1   \n",
      "british-irish       4      0       3        6         5         0       0   \n",
      "cajun_creole       11      1       1       21         1         0       2   \n",
      "chinese             2      0       8        4         0        18      13   \n",
      "filipino            2      0       1        6         0         3       1   \n",
      "french            405      8       4      114         0         1       0   \n",
      "greek               6    179       4       49         0         0       0   \n",
      "indian              5      8     664       10         0        10       0   \n",
      "italian            89     14       3     1766         1         0       1   \n",
      "jamaican            1      0       9        4        81         1       1   \n",
      "japanese            7      1      24        7         0       246       5   \n",
      "korean              0      0       1        2         0         9     164   \n",
      "mexican            15      5       7       41         1         0       1   \n",
      "moroccan            6      6      14       11         0         0       1   \n",
      "russian            19      1       1       19         0         1       0   \n",
      "southern_us        44      1       2       46         6         5       0   \n",
      "spanish            27      5       1       48         0         0       0   \n",
      "thai                1      0      10        1         1         3       3   \n",
      "vietnamese          2      1       1        4         0         3       2   \n",
      "\n",
      "               mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
      "brazilian            2         1        2           67        5     1   \n",
      "british-irish       13         0        1            8        5     4   \n",
      "cajun_creole        10         0        2           47        2     0   \n",
      "chinese              5         1        2            7        0    16   \n",
      "filipino             9         0        1           11        0     8   \n",
      "french               4         1        6           48       13     0   \n",
      "greek                2         4        1            5        7     0   \n",
      "indian              15        13        1            5        2    11   \n",
      "italian             14         4        1           36       10     1   \n",
      "jamaican             3         0        1           12        0     1   \n",
      "japanese             3         0        0            4        0     3   \n",
      "korean               3         0        0            3        1     2   \n",
      "mexican           1539         1        0           26       13     1   \n",
      "moroccan             1       164        0            3        2     0   \n",
      "russian              7         2       55            7        1     1   \n",
      "southern_us         37         0        4          817        5     1   \n",
      "spanish             19         4        1           16      118     1   \n",
      "thai                 4         1        0            3        1   303   \n",
      "vietnamese           5         0        0            1        0    35   \n",
      "\n",
      "               vietnamese  \n",
      "brazilian               0  \n",
      "british-irish           0  \n",
      "cajun_creole            0  \n",
      "chinese                10  \n",
      "filipino                6  \n",
      "french                  0  \n",
      "greek                   0  \n",
      "indian                  1  \n",
      "italian                 0  \n",
      "jamaican                1  \n",
      "japanese                5  \n",
      "korean                  2  \n",
      "mexican                 0  \n",
      "moroccan                1  \n",
      "russian                 0  \n",
      "southern_us             1  \n",
      "spanish                 0  \n",
      "thai                   28  \n",
      "vietnamese            121  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "cats_BI = ['brazilian', 'british-irish', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "cm = confusion_matrix(y_dev_non_BI, preds_non_BI)\n",
    "cmdf = pd.DataFrame(cm, index = cats_BI, columns = cats_BI)\n",
    "print(cmdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Contiential food Source\n",
    "\n",
    "Here I attempt to improve the model by adding which continent the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "continents ={'brazilian':\"America\"\n",
    "             , 'british': \"Europe\"\n",
    "             , 'cajun_creole': \"America\"\n",
    "             , 'chinese': \"Asia\"\n",
    "             , 'filipino': \"Asia\"\n",
    "             , 'french': \"Europe\"\n",
    "             , 'greek': \"Europe\"\n",
    "             , 'indian': \"Asia\"\n",
    "             ,'irish': \"Europe\"\n",
    "             , 'italian': \"Europe\"\n",
    "             , 'jamaican': \"America\"\n",
    "             ,'japanese': \"Asia\"\n",
    "             , 'korean': \"Asia\"\n",
    "             , 'mexican': \"America\"\n",
    "             , 'moroccan': \"Africa\"\n",
    "             , 'russian': \"Europe\"\n",
    "             , 'southern_us':\"America\"\n",
    "             , 'spanish': \"Europe\"\n",
    "             , 'thai': \"Asia\"\n",
    "             ,'vietnamese': \"Asia\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asia'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continents[\"chinese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Label                                             Recipe Continent  \\\n0  southern_us                               peanuts, salt, water   America   \n1  southern_us  large eggs, all-purpose flour, baking soda, bu...   America   \n2      mexican  pepper, jalapeno chilies, salt, oil, masa hari...   America   \n3      chinese  peeled fresh ginger, noodles, hoisin sauce, sa...      Asia   \n4      italian  short-grain rice, hot water, soy sauce, scalli...    Europe   \n\n   Length  \n0       3  \n1       9  \n2      17  \n3       9  \n4       8  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train = pd.DataFrame({\"Recipe\": X_train, \"Label\": y_train})\n",
    "df_X_train[\"Continent\"] = [continents[label] for label in df_X_train[\"Label\"]]\n",
    "df_X_train[\"Length\"] = df_X_train[\"Recipe\"].apply(lambda r: len(r.split(\", \")))\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Label                                             Recipe Continent  \\\n47     irish  ground black pepper, gran marnier, strawberrie...    Europe   \n62     irish  ground black pepper, garlic, dried thyme, butt...    Europe   \n72     irish  lemon curd, whipping cream, mint sprigs, orang...    Europe   \n81     irish  baking soda, salt, russet potatoes, buttermilk...    Europe   \n104  british  pudding, fresh rosemary, cracked black pepper,...    Europe   \n\n     Length  \n47        6  \n62       11  \n72        8  \n81        6  \n104       6  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BI_train = df_X_train.loc[BI_indexes]\n",
    "BI_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": null,
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "Hipple.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
