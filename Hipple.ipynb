{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y =[]\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              brazilian  british  cajun_creole  chinese  filipino  french  \\\nbrazilian            46        1            11        0         1       4   \nbritish               0       72             4        1         2      33   \ncajun_creole          0        7           284        1         0       6   \nchinese               0        3             6      583         8       1   \nfilipino              4        4             4       42        73       3   \nfrench                1       29            15        1         1     347   \ngreek                 0        5             6        0         0       8   \nindian                2       13             2        3         1       4   \nirish                 1       17             5        0         3      21   \nitalian               2       31            29        2         0     145   \njamaican              4        3             4        1         2       1   \njapanese              1        1             0       48         1      12   \nkorean                1        0             0       54         0       1   \nmexican               8        9            15        3         6       7   \nmoroccan              0        4             3        0         0       2   \nrussian               0       12             6        0         3      23   \nsouthern_us           3       18           108        9         8      32   \nspanish               0        6            13        1         0      28   \nthai                  4        0             1       30         2       0   \nvietnamese            3        1             0       18         2       2   \n\n              greek  indian  irish  italian  jamaican  japanese  korean  \\\nbrazilian         0       7      0        5         1         0       0   \nbritish           1       6      9        7         1         1       1   \ncajun_creole      0       4      1       16         1         1       1   \nchinese           0       4      0        1         2        11      12   \nfilipino          0       3      0        5         0         1       4   \nfrench           19       2      8      102         1         1       0   \ngreek           165       5      0       38         0         0       0   \nindian           13     653      1        5         1         2       0   \nirish             1       1     62        9         0         0       0   \nitalian          40       1      4     1559         0         0       0   \njamaican          0       7      0        3        74         0       1   \njapanese          1      29      1        2         0       208      10   \nkorean            0       0      0        1         0         6     145   \nmexican           4      16      2       20         1         0       2   \nmoroccan          9      14      1        1         0         1       0   \nrussian           2       2      3       10         0         0       1   \nsouthern_us       5      11      5       48         7         3       1   \nspanish           6       0      0       35         1         0       0   \nthai              1      11      0        1         0         3       2   \nvietnamese        0       0      0        3         0         3       2   \n\n              mexican  moroccan  russian  southern_us  spanish  thai  \\\nbrazilian           6         1        1           19       10     8   \nbritish             1         0        2           63        2     0   \ncajun_creole        8         1        1           40        4     0   \nchinese             2         1        2            6        0    15   \nfilipino            4         0        1           28        2    11   \nfrench              7         3        2           76       20     0   \ngreek               1        11        3            9        7     0   \nindian              5        19        1           15        0    17   \nirish               1         2        0           48        3     1   \nitalian            16         9        2           84       34     4   \njamaican            2         4        0           15        0     2   \njapanese            2         0        1           14        0    10   \nkorean              2         0        0            8        0     2   \nmexican          1464         3        1           72       22    10   \nmoroccan            1       167        0            7        3     1   \nrussian            10         2       39           18        2     0   \nsouthern_us        40         2        6          728       12     6   \nspanish            12         3        0           25      115     2   \nthai                2         1        0            2        0   311   \nvietnamese          1         1        0            1        0    57   \n\n              vietnamese  \nbrazilian              0  \nbritish                0  \ncajun_creole           0  \nchinese               13  \nfilipino               1  \nfrench                 1  \ngreek                  0  \nindian                 1  \nirish                  0  \nitalian                1  \njamaican               0  \njapanese               1  \nkorean                 1  \nmexican                3  \nmoroccan               1  \nrussian                0  \nsouthern_us            4  \nspanish                0  \nthai                  20  \nvietnamese           101  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_conf_matrix = pd.DataFrame(confusion_matrix(y_dev, predictions_base), columns = cats, index = cats)\n",
    "base_conf_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "              brazilian  british  cajun_creole  chinese  filipino  french  \\\n",
    "brazilian            46        1            11        0         1       4   \n",
    "british               0       72             4        1         2      33   \n",
    "cajun_creole          0        7           284        1         0       6   \n",
    "chinese               0        3             6      583         8       1   \n",
    "filipino              4        4             4       42        73       3   \n",
    "french                1       29            15        1         1     347   \n",
    "greek                 0        5             6        0         0       8   \n",
    "indian                2       13             2        3         1       4   \n",
    "irish                 1       17             5        0         3      21   \n",
    "italian               2       31            29        2         0     145   \n",
    "jamaican              4        3             4        1         2       1   \n",
    "japanese              1        1             0       48         1      12   \n",
    "korean                1        0             0       54         0       1   \n",
    "mexican               8        9            15        3         6       7   \n",
    "moroccan              0        4             3        0         0       2   \n",
    "russian               0       12             6        0         3      23   \n",
    "southern_us           3       18           108        9         8      32   \n",
    "spanish               0        6            13        1         0      28   \n",
    "thai                  4        0             1       30         2       0   \n",
    "vietnamese            3        1             0       18         2       2   \n",
    "\n",
    "              greek  indian  irish  italian  jamaican  japanese  korean  \\\n",
    "brazilian         0       7      0        5         1         0       0   \n",
    "british           1       6      9        7         1         1       1   \n",
    "cajun_creole      0       4      1       16         1         1       1   \n",
    "chinese           0       4      0        1         2        11      12   \n",
    "filipino          0       3      0        5         0         1       4   \n",
    "french           19       2      8      102         1         1       0   \n",
    "greek           165       5      0       38         0         0       0   \n",
    "indian           13     653      1        5         1         2       0   \n",
    "irish             1       1     62        9         0         0       0   \n",
    "italian          40       1      4     1559         0         0       0   \n",
    "jamaican          0       7      0        3        74         0       1   \n",
    "japanese          1      29      1        2         0       208      10   \n",
    "korean            0       0      0        1         0         6     145   \n",
    "mexican           4      16      2       20         1         0       2   \n",
    "moroccan          9      14      1        1         0         1       0   \n",
    "russian           2       2      3       10         0         0       1   \n",
    "southern_us       5      11      5       48         7         3       1   \n",
    "spanish           6       0      0       35         1         0       0   \n",
    "thai              1      11      0        1         0         3       2   \n",
    "vietnamese        0       0      0        3         0         3       2   \n",
    "\n",
    "              mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
    "brazilian           6         1        1           19       10     8   \n",
    "british             1         0        2           63        2     0   \n",
    "cajun_creole        8         1        1           40        4     0   \n",
    "chinese             2         1        2            6        0    15   \n",
    "filipino            4         0        1           28        2    11   \n",
    "french              7         3        2           76       20     0   \n",
    "greek               1        11        3            9        7     0   \n",
    "indian              5        19        1           15        0    17   \n",
    "irish               1         2        0           48        3     1   \n",
    "italian            16         9        2           84       34     4   \n",
    "jamaican            2         4        0           15        0     2   \n",
    "japanese            2         0        1           14        0    10   \n",
    "korean              2         0        0            8        0     2   \n",
    "mexican          1464         3        1           72       22    10   \n",
    "moroccan            1       167        0            7        3     1   \n",
    "russian            10         2       39           18        2     0   \n",
    "southern_us        40         2        6          728       12     6   \n",
    "spanish            12         3        0           25      115     2   \n",
    "thai                2         1        0            2        0   311   \n",
    "vietnamese          1         1        0            1        0    57   \n",
    "\n",
    "              vietnamese  \n",
    "brazilian              0  \n",
    "british                0  \n",
    "cajun_creole           0  \n",
    "chinese               13  \n",
    "filipino               1  \n",
    "french                 1  \n",
    "greek                  0  \n",
    "indian                 1  \n",
    "irish                  0  \n",
    "italian                1  \n",
    "jamaican               0  \n",
    "japanese               1  \n",
    "korean                 1  \n",
    "mexican                3  \n",
    "moroccan               1  \n",
    "russian                0  \n",
    "southern_us            4  \n",
    "spanish                0  \n",
    "thai                  20  \n",
    "vietnamese           101  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'peanuts, salt, water',\n u'large eggs, all-purpose flour, baking soda, buttermilk, yellow corn meal, dried sage, double-acting baking powder, unsalted butter, salt',\n u'pepper, jalapeno chilies, salt, oil, masa harina, lime juice, chili powder, cayenne pepper, onions, water, Mexican oregano, salsa, ground beef, fresh cilantro, garlic, peanut oil, cumin',\n u'peeled fresh ginger, noodles, hoisin sauce, salt, water, green onions, five-spice powder, pork tenderloin, peanut oil',\n u'short-grain rice, hot water, soy sauce, scallions, ponzu, cabbage, fresh ginger, shrimp']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print (\"peanut oil\" in cv.get_feature_names())\n",
    "print (\"oil\" in cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "The count vectorizor is splitting on every space, not just on the \", \" that we'd prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'light_brown_sugar, lemongrass, jalapeno_chilies, cilantro_leaves, vegan_mayonnaise, soy_sauce, mo_hanh, cilantro, ground_white_pepper, liquid_aminos, pickled_carrots, vegetable_oil, ground_coriander, baguette, extra_firm_tofu, garlic, cucumber',\n u'ground_cinnamon, soy_sauce, raisins, brown_sugar, water, salt, chestnuts, pinenuts, jujube, sweet_rice, sesame_oil, walnuts',\n u'dry_white_wine, dry_bread_crumbs, olive_oil, garlic, fresh_parsley, lemon, fillets, ground_black_pepper, salt',\n u'filet_mignon, large_eggs, puff_pastry_sheets, veal_demi-glace, shallots, minced_garlic, mushrooms, Madeira, unsalted_butter, gorgonzola',\n u'scallions, light_coconut_milk, salt, water, basmati_rice']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_spaces(recipe_list):\n",
    "    return \", \".join(map(lambda s: s.replace(\" \", \"_\"), recipe_list.split(\", \")))\n",
    "\n",
    "X_train_nospc = list(map(remove_spaces, X_train))\n",
    "X_dev_nospc = list(map(remove_spaces, X_dev))\n",
    "X_train_nospc[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cv_nospc = CountVectorizer()\n",
    "tf_X_train_nospc= cv_nospc.fit_transform(X_train_nospc)\n",
    "tf_X_dev_nospc= cv_nospc.transform(X_dev_nospc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#[print(feat) for feat in cv_nospc.get_feature_names() if \"_\" in feat]\n",
    "\n",
    "#fails = [feat for feat in cv_nospc.get_feature_names() if feat[0] == \"_\"]\n",
    "#print fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(tf_X_train_nospc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71755656988693173"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_dev, mnb.predict(tf_X_dev_nospc),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Well, taking out the spaces was no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Adding number of ingredients as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Preprocess step for working adjusting the ingredients list in comma\n",
    "    seperated format\n",
    "    \"\"\"\n",
    "\n",
    "    # Add number of ingredients as feature\n",
    "    s = s.lower()\n",
    "    num_ingredients = len(s.split(\", \"))\n",
    "    s += \", \" + str(num_ingredients)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72357435544647797"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_count = CountVectorizer(preprocessor = custom_preprocessor)\n",
    "mnb_with_count = MultinomialNB()\n",
    "\n",
    "tf_X_train_count = cv_count.fit_transform(X_train)\n",
    "tf_X_dev_count = cv_count.transform(X_dev)\n",
    "mnb_with_count.fit(tf_X_train_count, y_train) \n",
    "preds_with_count = mnb_with_count.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77598079133209752"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(tf_X_train_count, y_train)\n",
    "lr_preds_with_count = lr.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, lr_preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Margininal improvement if any.\n",
    "Including the remove spaces function in the prepropocessor only hurts the accuracy by about 1-2% for both mnb and lr models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def custom_tokenizer_2(string):\n",
    "    result = []\n",
    "    \n",
    "    #overall note: the point of sorting the ingredients before adding\n",
    "    #them to the list is to prevent duplicates that are just flipped\n",
    "    #like \"unsalted butter\" and \"butter unsalted\"\n",
    "    \n",
    "    #create an empty list where we're going to put the ngrams\n",
    "    #where n = 1 so we can later create combinations of those\n",
    "    single_grams = []\n",
    "    \n",
    "    \n",
    "    for ingredient in string.split(', '):\n",
    "        for n in range(1,len(ingredient.split())+1):\n",
    "            grams = ngrams(ingredient.split(' '), n)\n",
    "            for gram in grams:\n",
    "                #if the length of the ngram we're looking at is 1,\n",
    "                #add it to our single grams list.\n",
    "                if n == 1:\n",
    "                    single_grams.append(gram[0])\n",
    "                result.append(\" \".join(sorted(list(gram))))\n",
    "    \n",
    "    #finally add every combination of the n = 1 ngrams\n",
    "    #so from ['unsalted butter', 'baking powder']\n",
    "    #we should be adding: 'butter unsalted', 'baking powder',\n",
    "    #'baking butter', 'baking unsalted', 'butter powder', 'powder unsalted'\n",
    "    for combo in combinations(single_grams, 2):\n",
    "        result.append(' '.join(sorted(list(combo))))\n",
    "    \n",
    "    #return the unique elements of this list\n",
    "    #since there will be plenty of duplicates\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(ingredients):\n",
    "    result = []\n",
    "    for ingredient in ingredients.split(', '):\n",
    "        temp = ingredient.lower()\n",
    "        \n",
    "        temp = re.sub(r'\\d+|&', '', temp)\n",
    "        temp = re.sub(r' +', ' ', temp)\n",
    "        temp = ' '.join(word for word in temp.split() if len(word)>2)\n",
    "        \n",
    "        result.append(\"\".join(temp))\n",
    "    \n",
    "    return \", \".join(result)\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    return string.split(', ') + re.split(', | ',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620776347546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.543565823723\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "                             \n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vectorize\", vectorizer), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "print(pipe.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n 'model__alpha': 1.0,\n 'model__class_prior': None,\n 'model__fit_prior': True,\n 'steps': [('vectorize',\n   CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n           dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n           ngram_range=(0, 2),\n           preprocessor=<function custom_preprocessor at 0x7f23a700c8c0>,\n           stop_words=None, strip_accents=None,\n           token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n           tokenizer=<function custom_tokenizer at 0x7f23a700c398>,\n           vocabulary=None)),\n  ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n 'vectorize': CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n         ngram_range=(0, 2),\n         preprocessor=<function custom_preprocessor at 0x7f23a700c8c0>,\n         stop_words=None, strip_accents=None,\n         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n         tokenizer=<function custom_tokenizer at 0x7f23a700c398>,\n         vocabulary=None),\n 'vectorize__analyzer': u'word',\n 'vectorize__binary': False,\n 'vectorize__decode_error': u'strict',\n 'vectorize__dtype': numpy.int64,\n 'vectorize__encoding': u'utf-8',\n 'vectorize__input': u'content',\n 'vectorize__lowercase': True,\n 'vectorize__max_df': 1.0,\n 'vectorize__max_features': None,\n 'vectorize__min_df': 1,\n 'vectorize__ngram_range': (0, 2),\n 'vectorize__preprocessor': <function __main__.custom_preprocessor>,\n 'vectorize__stop_words': None,\n 'vectorize__strip_accents': None,\n 'vectorize__token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n 'vectorize__tokenizer': <function __main__.custom_tokenizer>,\n 'vectorize__vocabulary': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'brazilian',\n u'british',\n u'cajun_creole',\n u'chinese',\n u'filipino',\n u'french',\n u'greek',\n u'indian',\n u'irish',\n u'italian',\n u'jamaican',\n u'japanese',\n u'korean',\n u'mexican',\n u'moroccan',\n u'russian',\n u'southern_us',\n u'spanish',\n u'thai',\n u'vietnamese'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Our biggest issues are southern_us and italian foods.\n",
    "\n",
    "Not with eachother, but Creole and southern are very simmilar and iatlian is also similar to a lot of stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "#British-Irish Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BI_train =1 \n",
    "BI_indexes = [True if y == u\"british\" or y == u\"irish\" else False for label in y_train]\n",
    "\n",
    "type(X_train)\n",
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-a5399927e72c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBI_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not list"
     ]
    }
   ],
   "source": [
    "set(y_train[BI_indexes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    28740\nTrue      1090\ndtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BI_indexes = [True if y == u\"british\" or y == u\"irish\" else False for y in y_train]\n",
    "BI_indexes = pd.Series(BI_indexes)\n",
    "BI_indexes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817\n"
     ]
    }
   ],
   "source": [
    "BI_y = pd.Series(y_train)[BI_indexes]\n",
    "BI_x = pd.Series(X_train)[BI_indexes]\n",
    "\n",
    "X_train_BI, X_dev_BI, y_train_BI, y_dev_BI = train_test_split(BI_x, BI_y)\n",
    "print len(X_train_BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772336911643\n",
      "0.78021978022\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  (\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774850605808\n",
      "0.776556776557\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": null,
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "Hipple.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
