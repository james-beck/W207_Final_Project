{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y =[]\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "              brazilian  british  cajun_creole  chinese  filipino  french  \\\n",
    "brazilian            46        1            11        0         1       4   \n",
    "british               0       72             4        1         2      33   \n",
    "cajun_creole          0        7           284        1         0       6   \n",
    "chinese               0        3             6      583         8       1   \n",
    "filipino              4        4             4       42        73       3   \n",
    "french                1       29            15        1         1     347   \n",
    "greek                 0        5             6        0         0       8   \n",
    "indian                2       13             2        3         1       4   \n",
    "irish                 1       17             5        0         3      21   \n",
    "italian               2       31            29        2         0     145   \n",
    "jamaican              4        3             4        1         2       1   \n",
    "japanese              1        1             0       48         1      12   \n",
    "korean                1        0             0       54         0       1   \n",
    "mexican               8        9            15        3         6       7   \n",
    "moroccan              0        4             3        0         0       2   \n",
    "russian               0       12             6        0         3      23   \n",
    "southern_us           3       18           108        9         8      32   \n",
    "spanish               0        6            13        1         0      28   \n",
    "thai                  4        0             1       30         2       0   \n",
    "vietnamese            3        1             0       18         2       2   \n",
    "\n",
    "              greek  indian  irish  italian  jamaican  japanese  korean  \\\n",
    "brazilian         0       7      0        5         1         0       0   \n",
    "british           1       6      9        7         1         1       1   \n",
    "cajun_creole      0       4      1       16         1         1       1   \n",
    "chinese           0       4      0        1         2        11      12   \n",
    "filipino          0       3      0        5         0         1       4   \n",
    "french           19       2      8      102         1         1       0   \n",
    "greek           165       5      0       38         0         0       0   \n",
    "indian           13     653      1        5         1         2       0   \n",
    "irish             1       1     62        9         0         0       0   \n",
    "italian          40       1      4     1559         0         0       0   \n",
    "jamaican          0       7      0        3        74         0       1   \n",
    "japanese          1      29      1        2         0       208      10   \n",
    "korean            0       0      0        1         0         6     145   \n",
    "mexican           4      16      2       20         1         0       2   \n",
    "moroccan          9      14      1        1         0         1       0   \n",
    "russian           2       2      3       10         0         0       1   \n",
    "southern_us       5      11      5       48         7         3       1   \n",
    "spanish           6       0      0       35         1         0       0   \n",
    "thai              1      11      0        1         0         3       2   \n",
    "vietnamese        0       0      0        3         0         3       2   \n",
    "\n",
    "              mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
    "brazilian           6         1        1           19       10     8   \n",
    "british             1         0        2           63        2     0   \n",
    "cajun_creole        8         1        1           40        4     0   \n",
    "chinese             2         1        2            6        0    15   \n",
    "filipino            4         0        1           28        2    11   \n",
    "french              7         3        2           76       20     0   \n",
    "greek               1        11        3            9        7     0   \n",
    "indian              5        19        1           15        0    17   \n",
    "irish               1         2        0           48        3     1   \n",
    "italian            16         9        2           84       34     4   \n",
    "jamaican            2         4        0           15        0     2   \n",
    "japanese            2         0        1           14        0    10   \n",
    "korean              2         0        0            8        0     2   \n",
    "mexican          1464         3        1           72       22    10   \n",
    "moroccan            1       167        0            7        3     1   \n",
    "russian            10         2       39           18        2     0   \n",
    "southern_us        40         2        6          728       12     6   \n",
    "spanish            12         3        0           25      115     2   \n",
    "thai                2         1        0            2        0   311   \n",
    "vietnamese          1         1        0            1        0    57   \n",
    "\n",
    "              vietnamese  \n",
    "brazilian              0  \n",
    "british                0  \n",
    "cajun_creole           0  \n",
    "chinese               13  \n",
    "filipino               1  \n",
    "french                 1  \n",
    "greek                  0  \n",
    "indian                 1  \n",
    "irish                  0  \n",
    "italian                1  \n",
    "jamaican               0  \n",
    "japanese               1  \n",
    "korean                 1  \n",
    "mexican                3  \n",
    "moroccan               1  \n",
    "russian                0  \n",
    "southern_us            4  \n",
    "spanish                0  \n",
    "thai                  20  \n",
    "vietnamese           101  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'peanuts, salt, water',\n u'large eggs, all-purpose flour, baking soda, buttermilk, yellow corn meal, dried sage, double-acting baking powder, unsalted butter, salt',\n u'pepper, jalapeno chilies, salt, oil, masa harina, lime juice, chili powder, cayenne pepper, onions, water, Mexican oregano, salsa, ground beef, fresh cilantro, garlic, peanut oil, cumin',\n u'peeled fresh ginger, noodles, hoisin sauce, salt, water, green onions, five-spice powder, pork tenderloin, peanut oil',\n u'short-grain rice, hot water, soy sauce, scallions, ponzu, cabbage, fresh ginger, shrimp']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "The count vectorizor is splitting on every space, not just on the \", \" that we'd prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'light_brown_sugar, lemongrass, jalapeno_chilies, cilantro_leaves, vegan_mayonnaise, soy_sauce, mo_hanh, cilantro, ground_white_pepper, liquid_aminos, pickled_carrots, vegetable_oil, ground_coriander, baguette, extra_firm_tofu, garlic, cucumber',\n u'ground_cinnamon, soy_sauce, raisins, brown_sugar, water, salt, chestnuts, pinenuts, jujube, sweet_rice, sesame_oil, walnuts',\n u'dry_white_wine, dry_bread_crumbs, olive_oil, garlic, fresh_parsley, lemon, fillets, ground_black_pepper, salt',\n u'filet_mignon, large_eggs, puff_pastry_sheets, veal_demi-glace, shallots, minced_garlic, mushrooms, Madeira, unsalted_butter, gorgonzola',\n u'scallions, light_coconut_milk, salt, water, basmati_rice']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_spaces(recipe_list):\n",
    "    return \", \".join(map(lambda s: s.replace(\" \", \"_\"), recipe_list.split(\", \")))\n",
    "\n",
    "X_train_nospc = list(map(remove_spaces, X_train))\n",
    "X_dev_nospc = list(map(remove_spaces, X_dev))\n",
    "X_train_nospc[5000:5005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cv_nospc = CountVectorizer()\n",
    "tf_X_train_nospc= cv_nospc.fit_transform(X_train_nospc)\n",
    "tf_X_dev_nospc= cv_nospc.transform(X_dev_nospc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#[print(feat) for feat in cv_nospc.get_feature_names() if \"_\" in feat]\n",
    "\n",
    "#fails = [feat for feat in cv_nospc.get_feature_names() if feat[0] == \"_\"]\n",
    "#print fails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(tf_X_train_nospc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71755656988693173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_dev, mnb.predict(tf_X_dev_nospc),average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Well, taking out the spaces was no good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Adding number of ingredients as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(s):\n",
    "    \"\"\"\n",
    "    Preprocess step for working adjusting the ingredients list in comma\n",
    "    seperated format\n",
    "    \"\"\"\n",
    "\n",
    "    # Add number of ingredients as feature\n",
    "    s = s.lower()\n",
    "    num_ingredients = len(s.split(\", \"))\n",
    "    s += \", \" + str(num_ingredients)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72357435544647797"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_count = CountVectorizer(preprocessor = custom_preprocessor)\n",
    "mnb_with_count = MultinomialNB()\n",
    "\n",
    "tf_X_train_count = cv_count.fit_transform(X_train)\n",
    "tf_X_dev_count = cv_count.transform(X_dev)\n",
    "mnb_with_count.fit(tf_X_train_count, y_train) \n",
    "preds_with_count = mnb_with_count.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77598079133209752"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(tf_X_train_count, y_train)\n",
    "lr_preds_with_count = lr.predict(tf_X_dev_count)\n",
    "metrics.f1_score(y_dev, lr_preds_with_count, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Margininal improvement if any.\n",
    "Including the remove spaces function in the prepropocessor only hurts the accuracy by about 1-2% for both mnb and lr models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def custom_tokenizer_2(string):\n",
    "    result = []\n",
    "    \n",
    "    #overall note: the point of sorting the ingredients before adding\n",
    "    #them to the list is to prevent duplicates that are just flipped\n",
    "    #like \"unsalted butter\" and \"butter unsalted\"\n",
    "    \n",
    "    #create an empty list where we're going to put the ngrams\n",
    "    #where n = 1 so we can later create combinations of those\n",
    "    single_grams = []\n",
    "    \n",
    "    \n",
    "    for ingredient in string.split(', '):\n",
    "        for n in range(1,len(ingredient.split())+1):\n",
    "            grams = ngrams(ingredient.split(' '), n)\n",
    "            for gram in grams:\n",
    "                #if the length of the ngram we're looking at is 1,\n",
    "                #add it to our single grams list.\n",
    "                if n == 1:\n",
    "                    single_grams.append(gram[0])\n",
    "                result.append(\" \".join(sorted(list(gram))))\n",
    "    \n",
    "    #finally add every combination of the n = 1 ngrams\n",
    "    #so from ['unsalted butter', 'baking powder']\n",
    "    #we should be adding: 'butter unsalted', 'baking powder',\n",
    "    #'baking butter', 'baking unsalted', 'butter powder', 'powder unsalted'\n",
    "    for combo in combinations(single_grams, 2):\n",
    "        result.append(' '.join(sorted(list(combo))))\n",
    "    \n",
    "    #return the unique elements of this list\n",
    "    #since there will be plenty of duplicates\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def custom_preprocessor(ingredients):\n",
    "    result = []\n",
    "    for ingredient in ingredients.split(', '):\n",
    "        temp = ingredient.lower()\n",
    "        \n",
    "        temp = re.sub(r'\\d+|&', '', temp)\n",
    "        temp = re.sub(r' +', ' ', temp)\n",
    "        temp = ' '.join(word for word in temp.split() if len(word)>2)\n",
    "        \n",
    "        result.append(\"\".join(temp))\n",
    "    \n",
    "    return \", \".join(result)\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    return string.split(', ') + re.split(', | ',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786202735318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782105262301\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "                             \n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vectorize\", vectorizer), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "print(pipe.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'brazilian',\n u'british',\n u'cajun_creole',\n u'chinese',\n u'filipino',\n u'french',\n u'greek',\n u'indian',\n u'irish',\n u'italian',\n u'jamaican',\n u'japanese',\n u'korean',\n u'mexican',\n u'moroccan',\n u'russian',\n u'southern_us',\n u'spanish',\n u'thai',\n u'vietnamese'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Our biggest issues are southern_us and italian foods.\n",
    "\n",
    "Not with eachother, but Creole and southern are very simmilar and iatlian is also similar to a lot of stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# British-Irish Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "817\n"
     ]
    }
   ],
   "source": [
    "BI_indexes = [True if y == u\"british\" or y == u\"irish\" else False for y in y_train]\n",
    "BI_indexes = pd.Series(BI_indexes)\n",
    "\n",
    "BI_y = pd.Series(y_train)[BI_indexes]\n",
    "BI_x = pd.Series(X_train)[BI_indexes]\n",
    "\n",
    "X_train_BI, X_dev_BI, y_train_BI, y_dev_BI = train_test_split(BI_x, BI_y)\n",
    "print len(X_train_BI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    The dense transformer is required when the model being fit requires a\n",
    "    dense representation matrix, not a sparse one.\n",
    "    \"\"\"\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.783882783883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.78      0.89      0.83       163\n",
      "      irish       0.79      0.63      0.70       110\n",
      "\n",
      "avg / total       0.79      0.78      0.78       273\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778388145194\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  (\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n",
    "print(classification_report(y_dev_BI, preds_BI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798534798535\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.82      0.85      0.83       163\n",
      "      irish       0.76      0.73      0.74       110\n",
      "\n",
      "avg / total       0.80      0.80      0.80       273\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797713820408\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty = \"l2\", C = )\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_BI.fit(X_train_BI, y_train_BI)\n",
    "preds_BI = pipe_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(pipe_BI.score(X_dev_BI, y_dev_BI))\n",
    "print(classification_report(y_dev_BI, preds_BI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Now we have to train a model to predict the british-irish category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29830\n",
      "29830\n"
     ]
    }
   ],
   "source": [
    "def make_BI_labels(label):\n",
    "    if label == u\"british\" or label == u\"irish\":\n",
    "        return u\"BI\"\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "y_train_non_BI = pd.Series(y_train).map(make_BI_labels)\n",
    "y_dev_non_BI = pd.Series(y_dev).map(make_BI_labels)\n",
    "print len(y_train_non_BI)\n",
    "print len(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.807320997586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804034698328\n"
     ]
    }
   ],
   "source": [
    "# Fit a model to classifier everything with joined British and Irish\n",
    "model = LogisticRegression(penalty = \"l2\")\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer_2)\n",
    "\n",
    "pipe_non_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "pipe_non_BI.fit(X_train, y_train_non_BI)\n",
    "preds_non_BI = pipe_non_BI.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev_non_BI, preds_non_BI, average='weighted'))\n",
    "print(pipe_non_BI.score(X_dev, y_dev_non_BI))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800096784998\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.83      0.61      0.70       121\n",
      "     british       0.58      0.54      0.56       206\n",
      "cajun_creole       0.79      0.74      0.76       376\n",
      "     chinese       0.85      0.85      0.85       670\n",
      "    filipino       0.73      0.65      0.69       190\n",
      "      french       0.64      0.66      0.65       636\n",
      "       greek       0.79      0.72      0.75       258\n",
      "      indian       0.87      0.91      0.89       758\n",
      "       irish       0.59      0.57      0.58       175\n",
      "     italian       0.81      0.90      0.85      1963\n",
      "    jamaican       0.88      0.68      0.77       123\n",
      "    japanese       0.82      0.74      0.78       342\n",
      "      korean       0.86      0.77      0.82       221\n",
      "     mexican       0.91      0.93      0.92      1668\n",
      "    moroccan       0.84      0.78      0.81       215\n",
      "     russian       0.75      0.44      0.55       133\n",
      " southern_us       0.74      0.77      0.76      1056\n",
      "     spanish       0.67      0.45      0.53       247\n",
      "        thai       0.82      0.82      0.82       391\n",
      "  vietnamese       0.74      0.65      0.69       195\n",
      "\n",
      " avg / total       0.80      0.80      0.80      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# On the British-Irish predicted recipies, predict with the BI model.\n",
    "BI_prediction_indexes = [True if y == u\"BI\" else False for y in preds_non_BI]\n",
    "fill_in_BI = pipe_BI.predict(pd.Series(X_dev)[BI_prediction_indexes])\n",
    "\n",
    "# Reconnect the predictions for the final model.\n",
    "preds_refilled = pd.Series(preds_non_BI)\n",
    "preds_refilled[BI_prediction_indexes] = fill_in_BI\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds_refilled, average='weighted'))\n",
    "print(classification_report(y_dev, preds_refilled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cats_BI = ['BI', 'brazilian', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "cm = confusion_matrix(y_dev_non_BI, preds_non_BI)\n",
    "cmdf = pd.DataFrame(cm, index = cats_BI, columns = cats_BI)\n",
    "#print(cmdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Contiential food Source\n",
    "\n",
    "Here I attempt to improve the model by adding which continent the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "continents ={'brazilian':\"America\"\n",
    "             , 'british': \"Europe\"\n",
    "             , 'cajun_creole': \"America\"\n",
    "             , 'chinese': \"Asia\"\n",
    "             , 'filipino': \"Asia\"\n",
    "             , 'french': \"Europe\"\n",
    "             , 'greek': \"Europe\"\n",
    "             , 'indian': \"Asia\"\n",
    "             ,'irish': \"Europe\"\n",
    "             , 'italian': \"Europe\"\n",
    "             , 'jamaican': \"America\"\n",
    "             ,'japanese': \"Asia\"\n",
    "             , 'korean': \"Asia\"\n",
    "             , 'mexican': \"America\"\n",
    "             , 'moroccan': \"Africa\"\n",
    "             , 'russian': \"Europe\"\n",
    "             , 'southern_us':\"America\"\n",
    "             , 'spanish': \"Europe\"\n",
    "             , 'thai': \"Asia\"\n",
    "             ,'vietnamese': \"Asia\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asia'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continents[\"chinese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def find_canned_goods(recipe):\n",
    "    \"\"\"This function searches for anything that's canned in a recipie\"\"\"\n",
    "    if \"can\" in recipe.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Label                                             Recipe Continent  \\\n0  southern_us                               peanuts, salt, water   America   \n1  southern_us  large eggs, all-purpose flour, baking soda, bu...   America   \n2      mexican  pepper, jalapeno chilies, salt, oil, masa hari...   America   \n3      chinese  peeled fresh ginger, noodles, hoisin sauce, sa...      Asia   \n4      italian  short-grain rice, hot water, soy sauce, scalli...    Europe   \n\n   Length  Canned  \n0       3   False  \n1       9   False  \n2      17    True  \n3       9   False  \n4       8   False  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train = pd.DataFrame({\"Recipe\": X_train, \"Label\": y_train})\n",
    "df_X_train[\"Continent\"] = [continents[label] for label in df_X_train[\"Label\"]]\n",
    "df_X_train[\"Length\"] = df_X_train[\"Recipe\"].apply(lambda r: len(r.split(\", \")))\n",
    "df_X_train[\"Canned\"] = df_X_train[\"Recipe\"].apply(find_canned_goods)\n",
    "df_X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Label                                             Recipe Continent  \\\n47     irish  ground black pepper, gran marnier, strawberrie...    Europe   \n62     irish  ground black pepper, garlic, dried thyme, butt...    Europe   \n72     irish  lemon curd, whipping cream, mint sprigs, orang...    Europe   \n81     irish  baking soda, salt, russet potatoes, buttermilk...    Europe   \n104  british  pudding, fresh rosemary, cracked black pepper,...    Europe   \n\n     Length  \n47        6  \n62       11  \n72        8  \n81        6  \n104       6  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BI_train = df_X_train.loc[BI_indexes]\n",
    "BI_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                Canned               Length          \n                  mean       std       mean       std\nLabel                                                \nbrazilian     0.060694  0.239113   9.352601  5.045010\nbritish       0.056856  0.231761   9.720736  4.118912\ncajun_creole  0.090598  0.287160  12.582906  4.648506\nchinese       0.076385  0.265680  11.992511  4.097274\nfilipino      0.053097  0.224426  10.111504  3.969738\nfrench        0.046766  0.211190   9.835821  4.163521\ngreek         0.029444  0.169139  10.170120  3.725204\nindian        0.065479  0.247424  12.685078  4.969620\nirish         0.034553  0.182830   9.286585  3.751603\nitalian       0.050043  0.218051   9.898723  3.803273\njamaican      0.133995  0.341070  12.325062  4.949098\njapanese      0.055504  0.229068   9.693802  4.247837\nkorean        0.062397  0.242075  11.321839  3.829190\nmexican       0.146541  0.353685  10.843606  4.643087\nmoroccan      0.034653  0.183052  12.831683  4.819182\nrussian       0.058989  0.235935  10.258427  4.034438\nsouthern_us   0.133272  0.339921   9.679534  3.940439\nspanish       0.025606  0.158065  10.521563  4.232634\nthai          0.062718  0.242560  12.538328  4.401921\nvietnamese    0.106349  0.308529  12.709524  5.234993"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_train[[\"Label\", \"Canned\", \"Length\"]].groupby(\"Label\").agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "All of the categories are within 1 standard deviation of eachother from all the other categories.  Unlikely to be any predictive power in these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": null,
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "Hipple.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
