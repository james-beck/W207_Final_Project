{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Cooking?\n",
    "## W207 Final Project\n",
    "## James Beck, Samir Datta, Chris Hipple  \n",
    "\n",
    "The goal of this competition is to correctly classfiy the cuisine of a recipe given its ingredients. There are 20 different cuisine types from around the world to classify.\n",
    "\n",
    "Kaggle link: https://www.kaggle.com/c/whats-cooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X_test = []\n",
    "ID_test = []\n",
    "for item in data:\n",
    "    X_test.append(', '.join(item['ingredients']))\n",
    "    ID_test.append(item['id'])    \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipes in training data: 29830\n",
      "Number of recipes in development data: 9944\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of recipes in training data: \"+str(len(X_train)))\n",
    "print(\"Number of recipes in development data: \"+str(len(X_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First attempt\n",
    "\n",
    "Our first attempt to classify the recipes was to use the \"bag of words\" approach. We used the count vectorizer to create a sparse matrix of every word in the recipes, and fit a logistic regression model on the training data. We used this model to predict the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.76      0.55      0.63       121\n",
      "     british       0.59      0.33      0.43       206\n",
      "cajun_creole       0.78      0.70      0.74       376\n",
      "     chinese       0.79      0.85      0.82       670\n",
      "    filipino       0.71      0.54      0.61       190\n",
      "      french       0.59      0.64      0.61       636\n",
      "       greek       0.76      0.70      0.73       258\n",
      "      indian       0.85      0.89      0.87       758\n",
      "       irish       0.67      0.47      0.55       175\n",
      "     italian       0.80      0.90      0.85      1963\n",
      "    jamaican       0.81      0.70      0.75       123\n",
      "    japanese       0.82      0.69      0.75       342\n",
      "      korean       0.84      0.74      0.79       221\n",
      "     mexican       0.91      0.92      0.91      1668\n",
      "    moroccan       0.81      0.78      0.80       215\n",
      "     russian       0.66      0.40      0.50       133\n",
      " southern_us       0.69      0.77      0.73      1056\n",
      "     spanish       0.63      0.43      0.51       247\n",
      "        thai       0.78      0.77      0.78       391\n",
      "  vietnamese       0.68      0.58      0.63       195\n",
      "\n",
      " avg / total       0.78      0.78      0.78      9944\n",
      "\n",
      "f1=score: 0.775129698146\n",
      "%accuracy: 0.78077232502\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "tf_X_train = cv.fit_transform(X_train)\n",
    "tf_X_dev = cv.transform(X_dev)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(tf_X_train, y_train)\n",
    "predictions = lr.predict(tf_X_dev)\n",
    "\n",
    "print(classification_report(y_dev, predictions))\n",
    "print(\"f1=score: \"+str(metrics.f1_score(y_dev, predictions, average='weighted')))\n",
    "print(\"Accuracy: \"+str(np.mean(predictions==y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple approach to classification gave us an f1-score of .775 and an overall accuracy of .781."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ingredients: 2849\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique ingredients: \"+str(len(cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important ingredients for each cuisine:\n",
      "\n",
      "brazilian\n",
      "curry -2.29897902984\n",
      "açai 2.4226027732\n",
      "tapioca 2.61786032914\n",
      "manioc 3.04912543585\n",
      "cachaca 5.66428087204\n",
      "\n",
      "\n",
      "british\n",
      "worcestershire 2.27248086882\n",
      "marmite 2.67139865846\n",
      "mincemeat 2.70626914232\n",
      "haddock 2.72879032515\n",
      "stilton 4.81098136838\n",
      "\n",
      "\n",
      "cajun_creole\n",
      "mortadella 1.81332465096\n",
      "jambalaya 1.81889122476\n",
      "salami 1.99265358265\n",
      "creole 3.20484073611\n",
      "cajun 3.68107968744\n",
      "\n",
      "\n",
      "chinese\n",
      "mein 2.14816618602\n",
      "kimchi -2.3302530577\n",
      "mirin -2.65507624154\n",
      "mandarin 2.77164923395\n",
      "szechwan 2.81858166975\n",
      "\n",
      "\n",
      "filipino\n",
      "dogs 2.34387283812\n",
      "basil -2.35298612833\n",
      "glutinous 2.35926921964\n",
      "lumpia 2.83518421257\n",
      "calamansi 3.45331717906\n",
      "\n",
      "\n",
      "french\n",
      "swiss 2.37677521465\n",
      "niçoise 2.41997322075\n",
      "crepes 2.47666412423\n",
      "gruyère 2.48880633927\n",
      "gruyere 2.86732227335\n",
      "\n",
      "\n",
      "greek\n",
      "tahini 2.69162180603\n",
      "ouzo 2.84030054373\n",
      "phyllo 3.14357148945\n",
      "greek 3.33927999564\n",
      "feta 4.26967378331\n",
      "\n",
      "\n",
      "indian\n",
      "cardamom 2.38706614407\n",
      "yoghurt 2.45940191191\n",
      "masala 2.51177177124\n",
      "curry 2.91925879494\n",
      "tandoori 3.78903804841\n",
      "\n",
      "\n",
      "irish\n",
      "stout 2.3342516842\n",
      "guinness 2.52920407521\n",
      "brisket 2.6861770924\n",
      "corned 3.27981753517\n",
      "irish 4.56445475245\n",
      "\n",
      "\n",
      "italian\n",
      "spaghetti 2.83602568538\n",
      "grits -3.05294366181\n",
      "gnocchi 3.28047131586\n",
      "polenta 3.4078109814\n",
      "arborio 3.74391483473\n",
      "\n",
      "\n",
      "jamaican\n",
      "nutmeg 2.07608529216\n",
      "rum 2.4058691742\n",
      "thyme 3.18909552309\n",
      "allspice 3.35762570268\n",
      "jerk 5.65131357654\n",
      "\n",
      "\n",
      "japanese\n",
      "sake 3.17437027829\n",
      "soba 3.18805630886\n",
      "mirin 3.26689503499\n",
      "bonito 3.4398735495\n",
      "miso 3.85822923311\n",
      "\n",
      "\n",
      "korean\n",
      "butter -2.05861006898\n",
      "sesame 2.12459637575\n",
      "pinenuts 2.38963910126\n",
      "gochujang 3.13849270587\n",
      "kimchi 4.72585006848\n",
      "\n",
      "\n",
      "mexican\n",
      "mexican 2.96819851899\n",
      "enchilada 3.13845727476\n",
      "tequila 3.55554928296\n",
      "taco 3.98031051341\n",
      "tortillas 3.9907466151\n",
      "\n",
      "\n",
      "moroccan\n",
      "semolina 2.10661500726\n",
      "preserved 2.12366852025\n",
      "flower 2.45340025731\n",
      "couscous 3.0750779196\n",
      "harissa 3.21602374015\n",
      "\n",
      "\n",
      "russian\n",
      "cottage 2.00755206995\n",
      "dill 2.0491453181\n",
      "dillweed 2.22469351178\n",
      "sauerkraut 2.24200964608\n",
      "beets 3.05262772299\n",
      "\n",
      "\n",
      "southern_us\n",
      "lima 2.64556590694\n",
      "wafers 2.81553557447\n",
      "collard 2.9198920261\n",
      "eyed 3.74021635896\n",
      "grits 4.41972157697\n",
      "\n",
      "\n",
      "spanish\n",
      "pimenton 1.85423221484\n",
      "pimentos 1.90561558156\n",
      "sherry 1.9498840277\n",
      "chorizo 2.65239232888\n",
      "manchego 3.13513664984\n",
      "\n",
      "\n",
      "thai\n",
      "palm 1.75530362452\n",
      "hoisin -2.03538486003\n",
      "galangal 2.11902737978\n",
      "vietnamese -2.52340800961\n",
      "sticky 2.54869792119\n",
      "\n",
      "\n",
      "vietnamese\n",
      "maggi 1.92074296692\n",
      "tapioca 2.0878481687\n",
      "fish 2.12648923323\n",
      "cheese -2.17569068735\n",
      "vietnamese 2.39175067413\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most important ingredients for each cuisine:\\n\")\n",
    "\n",
    "largestWeightedWords = []\n",
    "largestWeightedIndeces = []\n",
    "cv_featurenames = cv.get_feature_names()\n",
    "\n",
    "for cat in range(20):\n",
    "    print(cats[cat])\n",
    "    weightIndeces = np.argsort(abs(lr.coef_[cat]))[-5:]\n",
    "    for index in weightIndeces:\n",
    "        weight = lr.coef_[cat][index]\n",
    "        \n",
    "        print(cv_featurenames[index] + \" \" + str(weight))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create list of most common ingedients based off simple text parser\n",
    "ingredient_freq = []\n",
    "for featurename in cv_featurenames:\n",
    "    i = 0\n",
    "    for recipe in X_train:\n",
    "        if featurename in recipe:\n",
    "            i +=1\n",
    "    ingredient_freq.append((featurename, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'in', 22462),\n",
       " (u'on', 22015),\n",
       " (u'ic', 21720),\n",
       " (u'la', 19673),\n",
       " (u'ro', 18848),\n",
       " (u'salt', 18421),\n",
       " (u'an', 18202),\n",
       " (u'oil', 15985),\n",
       " (u'lo', 15483),\n",
       " (u'pepper', 15270),\n",
       " (u'garlic', 13600),\n",
       " (u'st', 13226),\n",
       " (u'onion', 13159),\n",
       " (u'or', 13064),\n",
       " (u'to', 12805),\n",
       " (u'mi', 11928),\n",
       " (u'ice', 11366),\n",
       " (u'el', 10789),\n",
       " (u'fresh', 10203),\n",
       " (u'round', 9822),\n",
       " (u'ground', 9763),\n",
       " (u'de', 9648),\n",
       " (u'au', 8960),\n",
       " (u'red', 8875),\n",
       " (u'onions', 8762),\n",
       " (u'oliv', 8417),\n",
       " (u'olive', 8416),\n",
       " (u'sugar', 8412),\n",
       " (u'mo', 8377),\n",
       " (u'sauc', 7699),\n",
       " (u'sauce', 7671),\n",
       " (u'it', 7660),\n",
       " (u'black', 7614),\n",
       " (u'tom', 7491),\n",
       " (u'tomato', 7294),\n",
       " (u'water', 7135),\n",
       " (u'chee', 6967),\n",
       " (u'chees', 6963),\n",
       " (u'chicken', 6942),\n",
       " (u'butt', 6920),\n",
       " (u'cheese', 6890),\n",
       " (u'egg', 6890),\n",
       " (u'butter', 6739),\n",
       " (u'no', 6688),\n",
       " (u'all', 6557),\n",
       " (u'cho', 6523),\n",
       " (u'flour', 6238),\n",
       " (u'tomatoes', 6197),\n",
       " (u'gin', 6077),\n",
       " (u'green', 6069)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_sorted_by_freq = sorted(ingredient_freq, key=lambda tup: tup[1], reverse=True)\n",
    "ingredients_sorted_by_freq[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "\n",
    "Our next step was to have the vectorizer detect word pairs in addition to single words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=score: 0.77820171114\n",
      "Accuracy: 0.78268302494\n"
     ]
    }
   ],
   "source": [
    "cv_bigrams = CountVectorizer(ngram_range=(1,2))\n",
    "tf_X_train_bigrams = cv_bigrams.fit_transform(X_train)\n",
    "tf_X_dev_bigrams = cv_bigrams.transform(X_dev)\n",
    "\n",
    "lr_bigrams = LogisticRegression()\n",
    "lr_bigrams.fit(tf_X_train_bigrams, y_train)\n",
    "predictions = lr_bigrams.predict(tf_X_dev_bigrams)\n",
    "\n",
    "print(\"f1=score: \"+str(metrics.f1_score(y_dev, predictions, average='weighted')))\n",
    "print(\"Accuracy: \"+str(np.mean(predictions==y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brazilian\n",
      "manioc 1.81684210974\n",
      "manioc flour 1.81684210974\n",
      "black beans 1.8963307854\n",
      "tapioca flour 1.94343426701\n",
      "cachaca 4.42654489565\n",
      "\n",
      "\n",
      "british\n",
      "stilton cheese 1.58576911257\n",
      "jam 1.60918448539\n",
      "mincemeat 1.70117801631\n",
      "marmite 2.14242286153\n",
      "stilton 3.28159490246\n",
      "\n",
      "\n",
      "cajun_creole\n",
      "oil powdered 1.39534041956\n",
      "powder dried 1.50187637936\n",
      "cajun seasoning 1.67942575972\n",
      "creole 2.32921833546\n",
      "cajun 2.3873676481\n",
      "\n",
      "\n",
      "chinese\n",
      "szechwan peppercorns 1.58492318786\n",
      "kimchi -1.62362458022\n",
      "mandarin 1.80026540523\n",
      "sake -1.89346466114\n",
      "mirin -2.62748695135\n",
      "\n",
      "\n",
      "filipino\n",
      "tilapia 1.38693410006\n",
      "mirin -1.42974680294\n",
      "basil -1.49834913221\n",
      "calamansi 1.94418668915\n",
      "lumpia 1.97017120327\n",
      "\n",
      "\n",
      "french\n",
      "snails 1.60195331392\n",
      "grits -1.75811624417\n",
      "duck 1.76781095857\n",
      "pasta -1.77045743012\n",
      "cognac 1.99113545665\n",
      "\n",
      "\n",
      "greek\n",
      "phyllo 2.25904621531\n",
      "tahini 2.27620076599\n",
      "feta cheese 2.56303692104\n",
      "feta 2.63273494929\n",
      "greek 2.91312357351\n",
      "\n",
      "\n",
      "indian\n",
      "curds 1.95553708379\n",
      "tandoori 2.20226763204\n",
      "masala 2.23408928644\n",
      "curry 2.41600137731\n",
      "yoghurt 2.5569324714\n",
      "\n",
      "\n",
      "irish\n",
      "brisket 1.62399168802\n",
      "corned 1.64929451772\n",
      "corned beef 1.64929451772\n",
      "potatoes 2.08766721311\n",
      "irish 3.42819439415\n",
      "\n",
      "\n",
      "italian\n",
      "gnocchi 2.38416028031\n",
      "grits -2.53939486774\n",
      "mascarpone 2.586261533\n",
      "spaghetti 2.91986886476\n",
      "polenta 3.37725611771\n",
      "\n",
      "\n",
      "jamaican\n",
      "nutmeg 1.7417191799\n",
      "rum 1.99001680133\n",
      "allspice 2.84641393647\n",
      "thyme 2.92833062476\n",
      "jerk 3.95608932431\n",
      "\n",
      "\n",
      "japanese\n",
      "nori 2.29715638204\n",
      "dashi 2.52074750699\n",
      "sake 3.25246328211\n",
      "mirin 3.28183526961\n",
      "miso 3.53743665137\n",
      "\n",
      "\n",
      "korean\n",
      "eggs carrots 1.61012295175\n",
      "gochujang base 1.87625770878\n",
      "gochujang 1.87625770878\n",
      "pinenuts 2.12262919487\n",
      "kimchi 3.57471117032\n",
      "\n",
      "\n",
      "mexican\n",
      "salsa 2.42769819422\n",
      "taco 2.59635483945\n",
      "mexican 2.65869795187\n",
      "tequila 3.25734346443\n",
      "tortillas 3.42142600371\n",
      "\n",
      "\n",
      "moroccan\n",
      "lemon kosher 1.98469828928\n",
      "green tea 2.05966586757\n",
      "cinnamon 2.08661838155\n",
      "harissa 2.32639844989\n",
      "couscous 2.89469631431\n",
      "\n",
      "\n",
      "russian\n",
      "sour 1.21574355848\n",
      "cabbage 1.29575819935\n",
      "sauerkraut 1.6162959677\n",
      "dill 1.76339369286\n",
      "beets 2.79842218635\n",
      "\n",
      "\n",
      "southern_us\n",
      "biscuits 2.12138749618\n",
      "peaches 2.51371674222\n",
      "green tomatoes 3.05342863133\n",
      "sweet potatoes 3.34637272559\n",
      "grits 4.57476681211\n",
      "\n",
      "\n",
      "spanish\n",
      "sherry 1.55200008081\n",
      "saffron 1.59125878845\n",
      "manchego 1.72867682475\n",
      "manchego cheese 1.73389815715\n",
      "chorizo 2.33126946538\n",
      "\n",
      "\n",
      "thai\n",
      "sticky rice 1.52925213178\n",
      "curry paste 1.55096240727\n",
      "coconut milk 1.5708241851\n",
      "peanut 1.58470080106\n",
      "thai 1.82211649119\n",
      "\n",
      "\n",
      "vietnamese\n",
      "maggi 1.49812864414\n",
      "cheese -1.64093275042\n",
      "vietnamese 1.64719558715\n",
      "tapioca 1.78900773411\n",
      "fish sauce 1.90328377208\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "largestWeightedWords = []\n",
    "largestWeightedIndeces = []\n",
    "cv_bigram_featurenames = cv_bigrams.get_feature_names()\n",
    "\n",
    "\n",
    "for cat in range(20):\n",
    "    print(cats[cat])\n",
    "    weightIndeces = np.argsort(abs(lr_bigrams.coef_[cat]))[-5:]\n",
    "    for index in weightIndeces:\n",
    "        weight = lr_bigrams.coef_[cat][index]\n",
    "        \n",
    "        print(cv_bigram_featurenames[index] + \" \" + str(weight))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom tokenizer and preprocessor\n",
    "\n",
    "Our next step was to attempt to build a bustom tokenizer and preprocessor to remove some noise and keep only the most important and informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Our custom preprocessor removes features that are uninformative - \n",
    "#like numbers, unnecessary spaces, and words that are two characters long or less.\n",
    "def custom_preprocessor(ingredients):\n",
    "    result = []\n",
    "    for ingredient in ingredients.split(', '):\n",
    "        temp = ingredient.lower()\n",
    "        \n",
    "        #remove numbers\n",
    "        temp = re.sub(r'\\d+|&', '', temp)\n",
    "        #remove unnecessary spaces\n",
    "        temp = re.sub(r' +', ' ', temp)\n",
    "        #remove any words that are two characters or less\n",
    "        temp = ' '.join(word for word in temp.split() if len(word)>2)\n",
    "        \n",
    "        result.append(\"\".join(temp))\n",
    "    \n",
    "    return \", \".join(result)\n",
    "\n",
    "#our custom_tokenizer retuns every combination of every word\n",
    "#in the ingredinet list.\n",
    "#this increases the number of features by a lot, but also improves\n",
    "#accuracy.\n",
    "#\n",
    "#the logic behind this tokenizer is that two ingreidents may not be\n",
    "#informative on their own, but if seen together they may help to predict\n",
    "#a certain cuisine.\n",
    "\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    result = []\n",
    "    \n",
    "    #overall note: the point of sorting the ingredients before adding\n",
    "    #them to the list is to prevent duplicates that are just flipped\n",
    "    #like \"unsalted butter\" and \"butter unsalted\"\n",
    "    \n",
    "    #create an empty list where we're going to put the ngrams\n",
    "    #where n = 1 so we can later create combinations of those\n",
    "    single_grams = []\n",
    "    \n",
    "    \n",
    "    for ingredient in string.split(', '):\n",
    "        for n in range(1,len(ingredient.split())+1):\n",
    "            grams = ngrams(ingredient.split(' '), n)\n",
    "            for gram in grams:\n",
    "                #if the length of the ngram we're looking at is 1,\n",
    "                #add it to our single grams list.\n",
    "                if n == 1:\n",
    "                    single_grams.append(gram[0])\n",
    "                result.append(\" \".join(sorted(list(gram))))\n",
    "    \n",
    "    #finally add every combination of the n = 1 ngrams\n",
    "    #so from ['unsalted butter', 'baking powder']\n",
    "    #we should be adding: 'butter unsalted', 'baking powder',\n",
    "    #'baking butter', 'baking unsalted', 'butter powder', 'powder unsalted'\n",
    "    for combo in combinations(single_grams, 2):\n",
    "        result.append(' '.join(sorted(list(combo))))\n",
    "    \n",
    "    #return the unique elements of this list\n",
    "    #since there will be plenty of duplicates\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795135616269\n",
      "0.800583266291\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=\"l2\")\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "                             \n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vectorize\", vectorizer), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "print(pipe.score(X_dev, y_dev))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
