{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# What's Cooking?\n",
    "## W207 Final Project\n",
    "## James Beck, Samir Datta, Chris Hipple  \n",
    "\n",
    "\n",
    "![Kaggle link](https://www.kaggle.com/c/whats-cooking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "The goal of this competition is to correctly classfiy the cuisine of a recipe given its ingredients. There are 20 different cuisine types from around the world to classify.  It is hosted by the company Yummly, and all of the data provided is by them.  Yummly is a recipe aggregator website which delivers personalized recipe recommendations and searching.  The ability to more accurately classify recipes by their cuisine type would improve their product offering.\n",
    "\n",
    "\n",
    "# The Data\n",
    "\n",
    "## Description\n",
    "All data is provided by Yummly, and per the rules of the competition, no extraneous data may be brought in for the model.\n",
    "\n",
    "The data provided is a list of recipe ingredients with a label.  We do not have any metadata information such as the name of the recipe, where in the world it was submitted from, and all of the ingredients have been translated are in english. \n",
    "\n",
    "## Preperation\n",
    "\n",
    "The first task was to read the json list into python and seperate into train and dev splits.  We explored several options for preprocessors and vectorizer options to further prepare the data before modelling.  Some basic cleaning, such as lowercasing the ingredients was included in the preprocessor.  Below we take an initial look at the data, first fitting an exploratory model to see where our challenges will lie, and then digging into the data to see what we can find which will help us to improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cats = ['brazilian', 'british', 'cajun_creole', 'chinese', 'filipino', 'french', 'greek', 'indian', 'irish', 'italian', 'jamaican','japanese', 'korean', 'mexican', 'moroccan', 'russian', 'southern_us', 'spanish', 'thai', 'vietnamese']\n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for item in data:\n",
    "    X.append(', '.join(item['ingredients']))\n",
    "    y.append(item['cuisine'])    \n",
    "\n",
    "with open('train.json') as data_file:    \n",
    "    test_data = json.load(data_file)\n",
    "\n",
    "X_test = []\n",
    "ID_test = []\n",
    "for item in test_data:\n",
    "    X_test.append(', '.join(item['ingredients']))\n",
    "    ID_test.append(item['id'])    \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "scrolled": true,
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of recipes in training data: 29830\n",
      "Number of recipes in development data: 9944\n",
      "Number of recipes in test data: 39774\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of recipes in training data: \"+str(len(X_train)))\n",
    "print(\"Number of recipes in development data: \"+str(len(X_dev)))\n",
    "print(\"Number of recipes in test data: \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# First attempt\n",
    "\n",
    "Our first attempt to classify the recipes was to use the \"bag of words\" approach. We used the count vectorizer to create a sparse matrix of every word in the recipes, and fit a logistic regression model on the training data. We used this model to predict the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.76      0.55      0.63       121\n",
      "     british       0.59      0.33      0.43       206\n",
      "cajun_creole       0.78      0.70      0.74       376\n",
      "     chinese       0.79      0.85      0.82       670\n",
      "    filipino       0.71      0.54      0.61       190\n",
      "      french       0.59      0.63      0.61       636\n",
      "       greek       0.76      0.70      0.73       258\n",
      "      indian       0.85      0.89      0.87       758\n",
      "       irish       0.67      0.47      0.55       175\n",
      "     italian       0.80      0.90      0.85      1963\n",
      "    jamaican       0.81      0.70      0.75       123\n",
      "    japanese       0.82      0.69      0.75       342\n",
      "      korean       0.84      0.74      0.79       221\n",
      "     mexican       0.91      0.92      0.91      1668\n",
      "    moroccan       0.81      0.78      0.80       215\n",
      "     russian       0.66      0.40      0.50       133\n",
      " southern_us       0.69      0.77      0.73      1056\n",
      "     spanish       0.63      0.43      0.51       247\n",
      "        thai       0.78      0.77      0.78       391\n",
      "  vietnamese       0.68      0.58      0.63       195\n",
      "\n",
      " avg / total       0.78      0.78      0.78      9944\n",
      "\n",
      "f1=score: 0.775022522654\n",
      "Accuracy: 0.780671761866\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "tf_X_train = cv.fit_transform(X_train)\n",
    "tf_X_dev = cv.transform(X_dev)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(tf_X_train, y_train)\n",
    "predictions = lr.predict(tf_X_dev)\n",
    "\n",
    "print(classification_report(y_dev, predictions))\n",
    "print(\"f1=score: \"+str(metrics.f1_score(y_dev, predictions, average='weighted')))\n",
    "print(\"Accuracy: \"+str(np.mean(predictions==y_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Our simple approach to classification gave us an f1-score of .775 and an overall accuracy of .781.\n",
    "\n",
    "One of the things that really sticks out for us in this first model is the very low recall score of British recipes.  Other cuisines also had low recall, as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ingredients: 2849\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique ingredients: \"+str(len(cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important ingredients for each cuisine:\n",
      "\n",
      "brazilian\n",
      "curry -2.29897910894\n",
      "açai 2.42260280202\n",
      "tapioca 2.6178603655\n",
      "manioc 3.0491254874\n",
      "cachaca 5.66428108383\n",
      "\n",
      "\n",
      "british\n",
      "worcestershire 2.2725271567\n",
      "marmite 2.67150973274\n",
      "mincemeat 2.70638328962\n",
      "haddock 2.72889219389\n",
      "stilton 4.81105108921\n",
      "\n",
      "\n",
      "cajun_creole\n",
      "mortadella 1.81332463325\n",
      "jambalaya 1.81889115661\n",
      "salami 1.99265353518\n",
      "creole 3.20484077806\n",
      "cajun 3.68107988305\n",
      "\n",
      "\n",
      "chinese\n",
      "mein 2.1481661817\n",
      "kimchi -2.33025305444\n",
      "mirin -2.65507624642\n",
      "mandarin 2.771649239\n",
      "szechwan 2.81858167377\n",
      "\n",
      "\n",
      "filipino\n",
      "dogs 2.34400654714\n",
      "basil -2.3530993541\n",
      "glutinous 2.35886364363\n",
      "lumpia 2.83547473376\n",
      "calamansi 3.45365349802\n",
      "\n",
      "\n",
      "french\n",
      "swiss 2.37624791163\n",
      "niçoise 2.42009817368\n",
      "crepes 2.47686289472\n",
      "gruyère 2.48861302158\n",
      "gruyere 2.86763157265\n",
      "\n",
      "\n",
      "greek\n",
      "tahini 2.69165878264\n",
      "ouzo 2.84016869992\n",
      "phyllo 3.14344962748\n",
      "greek 3.33915676849\n",
      "feta 4.26986694287\n",
      "\n",
      "\n",
      "indian\n",
      "cardamom 2.38706755008\n",
      "yoghurt 2.45940754006\n",
      "masala 2.5117701036\n",
      "curry 2.91925573564\n",
      "tandoori 3.78903276959\n",
      "\n",
      "\n",
      "irish\n",
      "stout 2.33424908922\n",
      "guinness 2.52920194013\n",
      "brisket 2.68617485726\n",
      "corned 3.27981465844\n",
      "irish 4.56445648499\n",
      "\n",
      "\n",
      "italian\n",
      "spaghetti 2.83602460373\n",
      "grits -3.05294241025\n",
      "gnocchi 3.28047194072\n",
      "polenta 3.40781478514\n",
      "arborio 3.74391602786\n",
      "\n",
      "\n",
      "jamaican\n",
      "nutmeg 2.07608473777\n",
      "rum 2.40586912351\n",
      "thyme 3.18909526488\n",
      "allspice 3.35762522824\n",
      "jerk 5.65131352425\n",
      "\n",
      "\n",
      "japanese\n",
      "sake 3.17437029222\n",
      "soba 3.18805630303\n",
      "mirin 3.26689503926\n",
      "bonito 3.43987353817\n",
      "miso 3.85822925004\n",
      "\n",
      "\n",
      "korean\n",
      "butter -2.05861007306\n",
      "sesame 2.12459641314\n",
      "pinenuts 2.38963909287\n",
      "gochujang 3.13849273409\n",
      "kimchi 4.72585009012\n",
      "\n",
      "\n",
      "mexican\n",
      "mexican 2.96819851544\n",
      "enchilada 3.13845725428\n",
      "tequila 3.55554929693\n",
      "taco 3.98031050834\n",
      "tortillas 3.99074660077\n",
      "\n",
      "\n",
      "moroccan\n",
      "semolina 2.10656332296\n",
      "preserved 2.12367156104\n",
      "flower 2.45336571674\n",
      "couscous 3.07518003496\n",
      "harissa 3.21594388803\n",
      "\n",
      "\n",
      "russian\n",
      "cottage 2.00755208751\n",
      "dill 2.04914529151\n",
      "dillweed 2.22469351902\n",
      "sauerkraut 2.2420096439\n",
      "beets 3.05262774365\n",
      "\n",
      "\n",
      "southern_us\n",
      "lima 2.64555315786\n",
      "wafers 2.81531044122\n",
      "collard 2.91973048765\n",
      "eyed 3.7403045078\n",
      "grits 4.41970996112\n",
      "\n",
      "\n",
      "spanish\n",
      "pimenton 1.85423221494\n",
      "pimentos 1.90561558118\n",
      "sherry 1.9498840278\n",
      "chorizo 2.65239232791\n",
      "manchego 3.13513665099\n",
      "\n",
      "\n",
      "thai\n",
      "palm 1.75517558323\n",
      "hoisin -2.0351059381\n",
      "galangal 2.1196745782\n",
      "vietnamese -2.52506406982\n",
      "sticky 2.55105727442\n",
      "\n",
      "\n",
      "vietnamese\n",
      "maggi 1.92074295142\n",
      "tapioca 2.08784796581\n",
      "fish 2.12648917838\n",
      "cheese -2.17569070858\n",
      "vietnamese 2.39175070021\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Most important ingredients for each cuisine:\\n\")\n",
    "\n",
    "largestWeightedWords = []\n",
    "largestWeightedIndeces = []\n",
    "cv_featurenames = cv.get_feature_names()\n",
    "\n",
    "for cat in range(20):\n",
    "    print(cats[cat])\n",
    "    weightIndeces = np.argsort(abs(lr.coef_[cat]))[-5:]\n",
    "    for index in weightIndeces:\n",
    "        weight = lr.coef_[cat][index]\n",
    "        \n",
    "        print(cv_featurenames[index] + \" \" + str(weight))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Create list of most common ingedients based off simple text parser\n",
    "ingredient_freq = []\n",
    "for featurename in cv_featurenames:\n",
    "    i = 0\n",
    "    for recipe in X_train:\n",
    "        if featurename in recipe:\n",
    "            i +=1\n",
    "    ingredient_freq.append((featurename, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'in', 22462),\n (u'on', 22015),\n (u'ic', 21720),\n (u'la', 19673),\n (u'ro', 18848),\n (u'salt', 18421),\n (u'an', 18202),\n (u'oil', 15985),\n (u'lo', 15483),\n (u'pepper', 15270),\n (u'garlic', 13600),\n (u'st', 13226),\n (u'onion', 13159),\n (u'or', 13064),\n (u'to', 12805),\n (u'mi', 11928),\n (u'ice', 11366),\n (u'el', 10789),\n (u'fresh', 10203),\n (u'round', 9822),\n (u'ground', 9763),\n (u'de', 9648),\n (u'au', 8960),\n (u'red', 8875),\n (u'onions', 8762),\n (u'oliv', 8417),\n (u'olive', 8416),\n (u'sugar', 8412),\n (u'mo', 8377),\n (u'sauc', 7699),\n (u'sauce', 7671),\n (u'it', 7660),\n (u'black', 7614),\n (u'tom', 7491),\n (u'tomato', 7294),\n (u'water', 7135),\n (u'chee', 6967),\n (u'chees', 6963),\n (u'chicken', 6942),\n (u'butt', 6920),\n (u'cheese', 6890),\n (u'egg', 6890),\n (u'butter', 6739),\n (u'no', 6688),\n (u'all', 6557),\n (u'cho', 6523),\n (u'flour', 6238),\n (u'tomatoes', 6197),\n (u'gin', 6077),\n (u'green', 6069)]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients_sorted_by_freq = sorted(ingredient_freq, key=lambda tup: tup[1], reverse=True)\n",
    "ingredients_sorted_by_freq[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Bigrams\n",
    "\n",
    "Our next step was to have the vectorizer detect word pairs in addition to single words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1=score: 0.77820171114\n",
      "Accuracy: 0.78268302494\n"
     ]
    }
   ],
   "source": [
    "cv_bigrams = CountVectorizer(ngram_range=(1,2))\n",
    "tf_X_train_bigrams = cv_bigrams.fit_transform(X_train)\n",
    "tf_X_dev_bigrams = cv_bigrams.transform(X_dev)\n",
    "\n",
    "lr_bigrams = LogisticRegression()\n",
    "lr_bigrams.fit(tf_X_train_bigrams, y_train)\n",
    "predictions = lr_bigrams.predict(tf_X_dev_bigrams)\n",
    "\n",
    "print(\"f1=score: \"+str(metrics.f1_score(y_dev, predictions, average='weighted')))\n",
    "print(\"Accuracy: \"+str(np.mean(predictions==y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tilapia 1.38695353957\n",
      "mirin -1.42973752698\n",
      "basil -1.49832955074\n",
      "calamansi 1.94421775293\n",
      "lumpia 1.97018948175\n",
      "\n",
      "\n",
      "french\n",
      "snails 1.60193180004\n",
      "grits -1.75813082953\n",
      "duck 1.76790366795\n",
      "pasta -1.77047745836\n",
      "cognac 1.9911612899\n",
      "\n",
      "\n",
      "greek\n",
      "phyllo 2.25896983829\n",
      "tahini 2.27594095227\n",
      "feta cheese 2.56310462361\n",
      "feta 2.63277686117\n",
      "greek 2.91301982708\n",
      "\n",
      "\n",
      "indian\n",
      "curds 1.95553711706\n",
      "tandoori 2.20226763025\n",
      "masala 2.2340892319\n",
      "curry 2.41600142299\n",
      "yoghurt 2.55693242774\n",
      "\n",
      "\n",
      "irish\n",
      "brisket 1.62398881977\n",
      "corned 1.64929876563\n",
      "corned beef 1.64929876563\n",
      "potatoes 2.08761701821\n",
      "irish 3.42817575402\n",
      "\n",
      "\n",
      "italian\n",
      "gnocchi 2.38422502236\n",
      "grits -2.53943645271\n",
      "mascarpone 2.58577356428\n",
      "spaghetti 2.91964769332\n",
      "polenta 3.37721102056\n",
      "\n",
      "\n",
      "jamaican\n",
      "nutmeg 1.74171922718\n",
      "rum 1.9900167825\n",
      "allspice 2.84641407624\n",
      "thyme 2.92833081638\n",
      "jerk 3.95608925945\n",
      "\n",
      "\n",
      "japanese\n",
      "nori 2.29715673956\n",
      "dashi 2.5207479706\n",
      "sake 3.25246432334\n",
      "mirin 3.28183409158\n",
      "miso 3.53743722457\n",
      "\n",
      "\n",
      "korean\n",
      "eggs carrots 1.61012297257\n",
      "gochujang base 1.8762577025\n",
      "gochujang 1.8762577025\n",
      "pinenuts 2.12262918482\n",
      "kimchi 3.5747111769\n",
      "\n",
      "\n",
      "mexican\n",
      "salsa 2.42764034764\n",
      "taco 2.59632343665\n",
      "mexican 2.65866516244\n",
      "tequila 3.25737229227\n",
      "tortillas 3.42138065719\n",
      "\n",
      "\n",
      "moroccan\n",
      "lemon kosher 1.98469828603\n",
      "green tea 2.05966586514\n",
      "cinnamon 2.08661836659\n",
      "harissa 2.32639845231\n",
      "couscous 2.89469630514\n",
      "\n",
      "\n",
      "russian\n",
      "sour 1.21574353043\n",
      "cabbage 1.29575825986\n",
      "sauerkraut 1.61629594685\n",
      "dill 1.76339368271\n",
      "beets 2.79842217247\n",
      "\n",
      "\n",
      "southern_us\n",
      "biscuits 2.12135089091\n",
      "peaches 2.51371514288\n",
      "green tomatoes 3.0533878188\n",
      "sweet potatoes 3.34632261552\n",
      "grits 4.57473760638\n",
      "\n",
      "\n",
      "spanish\n",
      "sherry 1.55200007242\n",
      "saffron 1.59125879772\n",
      "manchego 1.72867681501\n",
      "manchego cheese 1.73389814779\n",
      "chorizo 2.33126946962\n",
      "\n",
      "\n",
      "thai\n",
      "sticky 1.52922048984\n",
      "curry paste 1.55097056013\n",
      "coconut milk 1.57087174935\n",
      "peanut 1.58469138177\n",
      "thai 1.82214340832\n",
      "\n",
      "\n",
      "vietnamese\n",
      "maggi 1.49812850453\n",
      "cheese -1.6409324813\n",
      "vietnamese 1.64719566149\n",
      "tapioca 1.78900773589\n",
      "fish sauce 1.90328389137\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kimchi -1.6236245228\n",
      "mandarin 1.80026549182\n",
      "sake -1.89346463843\n",
      "mirin -2.62748693404\n",
      "\n",
      "\n",
      "filipino\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brazilian\n",
      "manioc flour 1.81681702844\n",
      "manioc 1.81681702844\n",
      "black beans 1.89634732893\n",
      "tapioca flour 1.94342987801\n",
      "cachaca 4.42659009273\n",
      "\n",
      "\n",
      "british\n",
      "stilton cheese 1.58576911268\n",
      "jam 1.60918448581\n",
      "mincemeat 1.70117801594\n",
      "marmite 2.14242286181\n",
      "stilton 3.28159490129\n",
      "\n",
      "\n",
      "cajun_creole\n",
      "oil powdered 1.39533660205\n",
      "powder dried 1.50187147299\n",
      "cajun seasoning 1.67941291376\n",
      "creole 2.32921401049\n",
      "cajun 2.38734973434\n",
      "\n",
      "\n",
      "chinese\n",
      "szechwan peppercorns 1.58492262895"
     ]
    }
   ],
   "source": [
    "largestWeightedWords = []\n",
    "largestWeightedIndeces = []\n",
    "cv_bigram_featurenames = cv_bigrams.get_feature_names()\n",
    "\n",
    "\n",
    "for cat in range(20):\n",
    "    print(cats[cat])\n",
    "    weightIndeces = np.argsort(abs(lr_bigrams.coef_[cat]))[-5:]\n",
    "    for index in weightIndeces:\n",
    "        weight = lr_bigrams.coef_[cat][index]\n",
    "        \n",
    "        print(cv_bigram_featurenames[index] + \" \" + str(weight))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "We notice that in our bigram model, many of the most important features for each cuisine are bigrams.  Even though the accuracy of our model was only slightly impacted, this tells us that including the extra bigram features is useful, and we should continue to move foward.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "## Custom tokenizer and preprocessor\n",
    "\n",
    "Our next step was to attempt to build a bustom tokenizer and preprocessor to remove some noise and keep only the most important and informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Our custom preprocessor removes features that are uninformative - \n",
    "#like numbers, unnecessary spaces, and words that are two characters long or less.\n",
    "def custom_preprocessor(ingredients):\n",
    "    result = []\n",
    "    for ingredient in ingredients.split(', '):\n",
    "        temp = ingredient.lower()\n",
    "        \n",
    "        #remove numbers\n",
    "        temp = re.sub(r'\\d+|&', '', temp)\n",
    "        #remove unnecessary spaces\n",
    "        temp = re.sub(r' +', ' ', temp)\n",
    "        #remove any words that are two characters or less\n",
    "        temp = ' '.join(word for word in temp.split() if len(word)>2)\n",
    "        \n",
    "        result.append(\"\".join(temp))\n",
    "    \n",
    "    return \", \".join(result)\n",
    "\n",
    "#our custom_tokenizer retuns every combination of every word\n",
    "#in the ingredinet list.\n",
    "#this increases the number of features by a lot, but also improves\n",
    "#accuracy.\n",
    "#\n",
    "#the logic behind this tokenizer is that two ingreidents may not be\n",
    "#informative on their own, but if seen together they may help to predict\n",
    "#a certain cuisine.\n",
    "\n",
    "\n",
    "def custom_tokenizer(string):\n",
    "    result = []\n",
    "    \n",
    "    #overall note: the point of sorting the ingredients before adding\n",
    "    #them to the list is to prevent duplicates that are just flipped\n",
    "    #like \"unsalted butter\" and \"butter unsalted\"\n",
    "    \n",
    "    #create an empty list where we're going to put the ngrams\n",
    "    #where n = 1 so we can later create combinations of those\n",
    "    single_grams = []\n",
    "    \n",
    "    \n",
    "    for ingredient in string.split(', '):\n",
    "        for n in range(1,len(ingredient.split())+1):\n",
    "            grams = ngrams(ingredient.split(' '), n)\n",
    "            for gram in grams:\n",
    "                #if the length of the ngram we're looking at is 1,\n",
    "                #add it to our single grams list.\n",
    "                if n == 1:\n",
    "                    single_grams.append(gram[0])\n",
    "                result.append(\" \".join(sorted(list(gram))))\n",
    "    \n",
    "    #finally add every combination of the n = 1 ngrams\n",
    "    #so from ['unsalted butter', 'baking powder']\n",
    "    #we should be adding: 'butter unsalted', 'baking powder',\n",
    "    #'baking butter', 'baking unsalted', 'butter powder', 'powder unsalted'\n",
    "    for combo in combinations(single_grams, 2):\n",
    "        result.append(' '.join(sorted(list(combo))))\n",
    "    \n",
    "    #return the unique elements of this list\n",
    "    #since there will be plenty of duplicates\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800080450523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.86      0.55      0.67       121\n",
      "     british       0.67      0.42      0.52       206\n",
      "cajun_creole       0.81      0.73      0.77       376\n",
      "     chinese       0.83      0.87      0.85       670\n",
      "    filipino       0.73      0.62      0.67       190\n",
      "      french       0.64      0.66      0.65       636\n",
      "       greek       0.80      0.69      0.74       258\n",
      "      indian       0.86      0.91      0.89       758\n",
      "       irish       0.72      0.49      0.58       175\n",
      "     italian       0.80      0.91      0.85      1963\n",
      "    jamaican       0.92      0.65      0.76       123\n",
      "    japanese       0.83      0.71      0.77       342\n",
      "      korean       0.87      0.75      0.80       221\n",
      "     mexican       0.88      0.94      0.91      1668\n",
      "    moroccan       0.87      0.79      0.82       215\n",
      "     russian       0.75      0.38      0.50       133\n",
      " southern_us       0.70      0.80      0.75      1056\n",
      "     spanish       0.70      0.45      0.55       247\n",
      "        thai       0.82      0.81      0.82       391\n",
      "  vietnamese       0.78      0.61      0.68       195\n",
      "\n",
      " avg / total       0.80      0.80      0.79      9944\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794368143097\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty=\"l2\")\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer,\n",
    "                             ngram_range = (0,2))\n",
    "                             \n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"vectorize\", vectorizer), (\"model\", model)])\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_dev)\n",
    "\n",
    "print(metrics.f1_score(y_dev, preds, average='weighted'))\n",
    "print(pipe.score(X_dev, y_dev))\n",
    "print(classification_report(y_dev, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Again, with this improved model, we see the low recall for the British.  This is likely one of our large sources of issue in the performance of our model, and we'll see to address it.  Below we take a look at the confusion matrix to see where our British results are getting mixed up, and other patterns of missclassification which may emerge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              brazilian  british  cajun_creole  chinese  filipino  french  \\\n",
      "brazilian            67        0             2        1         2       3   \n",
      "british               0       87             2        0         0      30   \n",
      "cajun_creole          1        2           274        1         0       7   \n",
      "chinese               0        1             2      583         8       2   \n",
      "filipino              3        1             0       18       118       2   \n",
      "french                0        7             9        2         1     420   \n",
      "greek                 0        1             0        0         1       7   \n",
      "indian                0        1             0        2         1       5   \n",
      "irish                 2       15             0        1         0      12   \n",
      "italian               0        2             3        4         2      60   \n",
      "jamaican              1        1             2        1         2       0   \n",
      "japanese              2        0             0       28         1      10   \n",
      "korean                0        0             0       24         4       1   \n",
      "mexican               1        1             3        1         1      12   \n",
      "moroccan              0        1             1        0         0       2   \n",
      "russian               0        5             0        0         1      23   \n",
      "southern_us           0        4            34        3         3      36   \n",
      "spanish               1        1             4        0         3      28   \n",
      "thai                  0        0             1       23         6       0   \n",
      "vietnamese            0        0             0       11         7       1   \n",
      "\n",
      "              greek  indian  irish  italian  jamaican  japanese  korean  \\\n",
      "brazilian         0       5      0        4         2         0       0   \n",
      "british           0       6     10       15         0         1       0   \n",
      "cajun_creole      0       1      1       25         0         1       0   \n",
      "chinese           0       3      1        4         0        15      14   \n",
      "filipino          0       1      1        7         0         3       1   \n",
      "french            5       5      4      110         0         1       0   \n",
      "greek           177       2      0       51         0         0       0   \n",
      "indian            7     690      0       11         0         2       0   \n",
      "irish             0       2     86       17         1         0       0   \n",
      "italian          20       6      4     1790         0         0       0   \n",
      "jamaican          0      10      1        2        80         0       0   \n",
      "japanese          1      28      0        9         0       244       4   \n",
      "korean            0       0      0        3         0        16     165   \n",
      "mexican           1       5      0       34         0         1       1   \n",
      "moroccan          4      15      1        6         1         0       0   \n",
      "russian           2       3      2       19         0         0       0   \n",
      "southern_us       1       4      8       56         3         3       1   \n",
      "spanish           3       2      1       55         0         0       0   \n",
      "thai              0      11      0        4         0         4       1   \n",
      "vietnamese        0       2      0        6         0         3       3   \n",
      "\n",
      "              mexican  moroccan  russian  southern_us  spanish  thai  \\\n",
      "brazilian          17         0        1           14        1     2   \n",
      "british             4         0        2           47        1     0   \n",
      "cajun_creole       14         0        0           46        3     0   \n",
      "chinese             8         0        0            9        0    12   \n",
      "filipino           11         0        0           15        2     2   \n",
      "french             11         1        3           48        9     0   \n",
      "greek               4         6        1            4        4     0   \n",
      "indian             13         7        1            8        0     9   \n",
      "irish               1         1        1           35        1     0   \n",
      "italian            22         4        0           36       10     0   \n",
      "jamaican            9         0        0           12        1     1   \n",
      "japanese            3         0        0            7        0     2   \n",
      "korean              4         0        0            2        0     1   \n",
      "mexican          1563         1        1           35        6     1   \n",
      "moroccan            5       169        0            8        2     0   \n",
      "russian            10         1       50           16        1     0   \n",
      "southern_us        40         1        5          847        6     1   \n",
      "spanish            19         3        2           12      112     1   \n",
      "thai                5         1        0            5        0   315   \n",
      "vietnamese          5         0        0            3        0    35   \n",
      "\n",
      "              vietnamese  \n",
      "brazilian              0  \n",
      "british                1  \n",
      "cajun_creole           0  \n",
      "chinese                8  \n",
      "filipino               5  \n",
      "french                 0  \n",
      "greek                  0  \n",
      "indian                 1  \n",
      "irish                  0  \n",
      "italian                0  \n",
      "jamaican               0  \n",
      "japanese               3  \n",
      "korean                 1  \n",
      "mexican                0  \n",
      "moroccan               0  \n",
      "russian                0  \n",
      "southern_us            0  \n",
      "spanish                0  \n",
      "thai                  15  \n",
      "vietnamese           119  \n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_dev, preds)\n",
    "cmdf = pd.DataFrame(cm, index = cats, columns = cats)\n",
    "print cmdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "From above in our confusion matrix, we also noticed that the british and irish often confused with eachother.  The other big take aways are that French and Southern_US cuisines are often misclassified as many other things.  We also noticed that in the classification report, these two lower support.  Hopefully by combining them, then classifying from the subgroup, it will improve our ability to predict both of them.\n",
    "\n",
    "First we explore the British and Irish confusions to see if we can fit a model that can accurately differenitate between the two, if only those two are in the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.782152230971\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.80      0.83      0.82       206\n",
      "      irish       0.80      0.76      0.78       175\n",
      "\n",
      "avg / total       0.80      0.80      0.80       381\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.80009165521\n"
     ]
    }
   ],
   "source": [
    "BI_indexes_train = [True if y == u\"british\" or y == u\"irish\" else False for y in y_train]\n",
    "BI_indexes_train = pd.Series(BI_indexes_train)\n",
    "\n",
    "BI_indexes_dev = [True if y == u\"british\" or y == u\"irish\" else False for y in y_dev]\n",
    "BI_indexes_dev = pd.Series(BI_indexes_dev)\n",
    "\n",
    "y_train_BI = pd.Series(y_train)[BI_indexes_train]\n",
    "X_train_BI = pd.Series(X_train)[BI_indexes_train]\n",
    "\n",
    "y_dev_BI = pd.Series(y_dev)[BI_indexes_dev]\n",
    "X_dev_BI = pd.Series(X_dev)[BI_indexes_dev]\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty = \"l2\", C = 1)\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer)\n",
    "\n",
    "pipe_BI = Pipeline([(\"vectorize\", vectorizer),\n",
    "                  #(\"to_dense\", DenseTransformer()),\n",
    "                  (\"model\", model)])\n",
    "\n",
    "\n",
    "gridsearch = False\n",
    "if gridsearch is True:\n",
    "    parameters = {\"model__penalty\": [\"l1\", \"l2\"],\n",
    "              \"model__C\": np.linspace(0.25, 1.5, 10)}\n",
    "\n",
    "    model_BI = GridSearchCV(pipe_BI, parameters)\n",
    "    model_BI.fit(X_train_BI, y_train_BI)\n",
    "    print model_BI.best_params_\n",
    "else:\n",
    "    model_BI = pipe_BI\n",
    "    model_BI.fit(X_train_BI, y_train_BI)\n",
    "\n",
    "preds_BI = model_BI.predict(X_dev_BI)\n",
    "\n",
    "print(metrics.f1_score(y_dev_BI, preds_BI, average='weighted'))\n",
    "print(GS.score(X_dev_BI, y_dev_BI))\n",
    "print(classification_report(y_dev_BI, preds_BI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "The high f1 and scores here show that this approach has some promise.  To integrate this idea into a large model, we developed a script which allowed us to systematically create subgroups of classifications in an ad-hoc manner and see which subgroups improved the overall performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def make_group_labels(label, group_name, group_cuisines):\n",
    "    if label in group_cuisines:\n",
    "        return group_name\n",
    "    else:\n",
    "        return label\n",
    "\n",
    "# For example\n",
    "BI_group = {\"cuisines\": [u\"british\", u\"irish\"], #which cuisines are included\n",
    "            \"group_name\": u\"BI\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "\n",
    "def train_ensemble_model(base_model, splits, X_train, y_train, X_dev, y_dev):\n",
    "    \"\"\"\n",
    "    This function accepts a list of dictionaries which define which supgroup splits to make, splits the training\n",
    "    data into those groups, and then makes sub-classifier models for each of them.  It then predicts against the\n",
    "    dev data to return an overall score.\n",
    "    \"\"\"\n",
    "\n",
    "    # First we must relabel the data to account for all of the splits\n",
    "    y_train_grouped = y_train\n",
    "    y_dev_grouped = y_dev\n",
    "    for split in splits:\n",
    "        y_train_grouped= pd.Series(y_train_grouped).map(lambda r:make_group_labels(r,\n",
    "                                                                                   split[\"group_name\"],\n",
    "                                                                                   split[\"cuisines\"]))\n",
    "\n",
    "        y_dev_grouped= pd.Series(y_dev_grouped).map(lambda r:make_group_labels(r,\n",
    "                                                                               split[\"group_name\"],\n",
    "                                                                               split[\"cuisines\"]))\n",
    "\n",
    "    # Next we train the base_model\n",
    "    print \"Fitting Base Model to classify into groups.\"\n",
    "    base_model.fit(X_train, y_train_grouped)\n",
    "\n",
    "    # Train the sub models for each group\n",
    "    for split in splits:\n",
    "        print(\"Training split model:\", split[\"group_name\"], \"for cuisines:\", split[\"cuisines\"])\n",
    "        # slice the data\n",
    "        train_split_indicies = pd.Series([True if y == split[\"group_name\"] else False for y in y_train_grouped])\n",
    "        X_train_split = pd.Series(X_train)[train_split_indicies]\n",
    "        y_train_split = pd.Series(y_train)[train_split_indicies]\n",
    "\n",
    "        dev_split_indicies = pd.Series([True if y == split[\"group_name\"] else False for y in y_dev_grouped])\n",
    "        X_dev_split = pd.Series(X_dev)[dev_split_indicies]\n",
    "        y_dev_split = pd.Series(y_dev)[dev_split_indicies]\n",
    "\n",
    "        if split[\"model\"] is None:\n",
    "            vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer)\n",
    "\n",
    "            classifier = LogisticRegression(penalty = \"l2\")\n",
    "            split_model = Pipeline([(\"vectorize\", vectorizer),\n",
    "                                    (\"model\", classifier)])\n",
    "        else:\n",
    "           split_model = split[\"model\"]\n",
    "\n",
    "        split_model.fit(X_train_split, y_train_split)\n",
    "        split_predictions = split_model.predict(X_dev_split)\n",
    "        split[\"model\"] = split_model\n",
    "\n",
    "        split[\"classification_report\"] = classification_report(y_dev_split, split_predictions)\n",
    "\n",
    "    print \"Creating final predictions.\"\n",
    "    preds = base_model.predict(X_dev)\n",
    "    for split in splits:\n",
    "        split_prediction_indicies = [True if y == split[\"group_name\"] else False for y in preds]\n",
    "        fill_in_predictions = split[\"model\"].predict(pd.Series(X_dev)[split_prediction_indicies])\n",
    "        preds = pd.Series(preds)\n",
    "        preds[split_prediction_indicies] = fill_in_predictions\n",
    "\n",
    "    print(metrics.f1_score(y_dev, preds, average=\"weighted\"))\n",
    "    print(classification_report(y_dev, preds))\n",
    "    return base_model, splits, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "Having defined the model, next we must apply it for splitting the British and Irish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800207994669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.83      0.61      0.70       121\n",
      "     british       0.57      0.54      0.56       206\n",
      "cajun_creole       0.79      0.74      0.76       376\n",
      "     chinese       0.85      0.85      0.85       670\n",
      "    filipino       0.73      0.65      0.69       190\n",
      "      french       0.64      0.66      0.65       636\n",
      "       greek       0.79      0.72      0.75       258\n",
      "      indian       0.87      0.91      0.89       758\n",
      "       irish       0.61      0.57      0.59       175\n",
      "     italian       0.81      0.90      0.85      1963\n",
      "    jamaican       0.88      0.68      0.77       123\n",
      "    japanese       0.82      0.74      0.78       342\n",
      "      korean       0.86      0.77      0.82       221\n",
      "     mexican       0.91      0.93      0.92      1668\n",
      "    moroccan       0.84      0.78      0.81       215\n",
      "     russian       0.75      0.44      0.55       133\n",
      " southern_us       0.74      0.77      0.76      1056\n",
      "     spanish       0.67      0.45      0.53       247\n",
      "        thai       0.82      0.82      0.82       391\n",
      "  vietnamese       0.74      0.65      0.69       195\n",
      "\n",
      " avg / total       0.80      0.80      0.80      9944\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    british       0.80      0.83      0.82       206\n",
      "      irish       0.80      0.76      0.78       175\n",
      "\n",
      "avg / total       0.80      0.80      0.80       381\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training split model:', u'BI', 'for cuisines:', [u'british', u'irish'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Base Model to classify into groups.\n"
     ]
    }
   ],
   "source": [
    "# Apply \n",
    "model = LogisticRegression(penalty = \"l2\", C = 1)\n",
    "\n",
    "vectorizer = CountVectorizer(preprocessor = custom_preprocessor,\n",
    "                             tokenizer = custom_tokenizer)\n",
    "base_model = Pipeline([(\"vectorize\", vectorizer),\n",
    "                       (\"model\", model)])\n",
    "\n",
    "BI_group = {\"cuisines\": [u\"british\", u\"irish\"], #which cuisines are included\n",
    "            \"group_name\": u\"BI\", # what sub-group this called by base model\n",
    "            \"model\": None} # The model which predicts the final cuisines\n",
    "\n",
    "splits = [BI_group]\n",
    "\n",
    "base_model, splits, preds = train_ensemble_model(base_model, splits, X_train, y_train, X_dev, y_dev)\n",
    "\n",
    "for split in splits:\n",
    "    print split[\"classification_report\"]\n",
    "    print \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "This model improves the accuracy and score on the dev data to above 0.8, which is a large milestone.\n",
    "\n",
    "Due to the special nature of our model, we had to define another function for creating the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.800207994669\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.83      0.61      0.70       121\n",
      "     british       0.57      0.54      0.56       206\n",
      "cajun_creole       0.79      0.74      0.76       376\n",
      "     chinese       0.85      0.85      0.85       670\n",
      "    filipino       0.73      0.65      0.69       190\n",
      "      french       0.64      0.66      0.65       636\n",
      "       greek       0.79      0.72      0.75       258\n",
      "      indian       0.87      0.91      0.89       758\n",
      "       irish       0.61      0.57      0.59       175\n",
      "     italian       0.81      0.90      0.85      1963\n",
      "    jamaican       0.88      0.68      0.77       123\n",
      "    japanese       0.82      0.74      0.78       342\n",
      "      korean       0.86      0.77      0.82       221\n",
      "     mexican       0.91      0.93      0.92      1668\n",
      "    moroccan       0.84      0.78      0.81       215\n",
      "     russian       0.75      0.44      0.55       133\n",
      " southern_us       0.74      0.77      0.76      1056\n",
      "     spanish       0.67      0.45      0.53       247\n",
      "        thai       0.82      0.82      0.82       391\n",
      "  vietnamese       0.74      0.65      0.69       195\n",
      "\n",
      " avg / total       0.80      0.80      0.80      9944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_ensemble_model(base_model, splits, data):\n",
    "    preds = base_model.predict(data)\n",
    "    for split in splits:\n",
    "        split_prediction_indexes = [True if y == split['group_name'] else False for y in preds]\n",
    "        fill_in_split = split['model'].predict(pd.Series(data)[split_prediction_indexes])\n",
    "\n",
    "        # Reconnect the predictions for the final model.\n",
    "        preds= pd.Series(preds)\n",
    "        preds[split_prediction_indexes] = fill_in_split\n",
    "    return preds\n",
    "\n",
    "dev_preds = predict_ensemble_model(base_model, [BI_group], X_dev)\n",
    "\n",
    "\n",
    "print(metrics.f1_score(y_dev, dev_preds, average='weighted'))\n",
    "print(classification_report(y_dev, dev_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "Overal, an accuracy of about 80% is quite good and we are proud of this score.  Throughout the process we looked at what "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "The last step required is to use our final model to predict against the test data and submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39774\n"
     ]
    }
   ],
   "source": [
    "test_predictions = predict_ensemble_model(base_model, [BI_group], X_test)\n",
    "print len(test_predictions)\n",
    "submission = pd.DataFrame({\"id\":ID_test, \"cuisine\":test_predictions})\n",
    "cols = [\"id\", \"cuisine\"]\n",
    "submission[cols].to_csv(\"submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide-type": "-"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Multiclass prediction problems are very difficult.  In this report, we made many improvements to our model by following a structured processing and trying many things.  We still realize that there is room for improvement, particularly in the realm of feature engineering.  More groups can easily be defined with our model, but we did not find any which improved the dev set accuracy beyond only using the British and Irish grouping.  Our best model ranked us to where we would have been tied for 213th out of 1388, which is just about at the 85th percentile.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": null,
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "final_project_submission.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
